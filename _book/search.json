[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data exploration toolkit for cultural data: structure, clean, visualize, and run a preliminary analysis",
    "section": "",
    "text": "Preface\nThis is a Quarto book"
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "1  Preface",
    "section": "",
    "text": "This is a Quarto book"
  },
  {
    "objectID": "parts/preliminaries/set_up.html",
    "href": "parts/preliminaries/set_up.html",
    "title": "2  Setting up",
    "section": "",
    "text": "Amazing set up instructions"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "7  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "parts/data_analysis/ch1_reading_data.html",
    "href": "parts/data_analysis/ch1_reading_data.html",
    "title": "4  Reading data",
    "section": "",
    "text": "The first thing we need to do is loading the data. This means taking the file where the data is currently stored and transfer that data here, in our working environment. As we are working with Python, this means transfering all the data in a Python object. Which object? There are Python libraries (code written by other developers) that are been specifically designed to the task of data analysis. One of these libraries, or (using the Python vocabulary) packages, is called pandas. So what we are going to do is reading our .csv file (coma separated file, where columns are separated by a coma) and storing the read information into a pandas DataFrame. How do we do all this? Using pandas and python specifically designed methods and functions.\n\n\nNameError: name 'list_to_html' is not defined\n\n\n\nimport pandas as pd\ndata_file = 'data/data.csv'\ndf = pd.read_csv(data_file)\nprint(type(df))\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\nNice! We managed to transfer our data into a Python object, specifically a pandas.core.frame.DataFrame, or simply (from now on) a DataFrame. However, a lot of things can go wrong when going from one data format to another, so it is a good idea to have a first look at the data.\n\n\n \n            \n                    \n                        \n                        \n                            Task: Have a first look at the data\n                        \n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Visualize the first 10 lines of data, just to check that everything looks \"ok\"\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    .head() method\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    pd.head(10), calling the head(10) method on the DataFrame df we will visualise the first 10 lines of the DataFrame (we wrote 10, but you can use whatever number you want). This method, as a matter of fact, shows you only the \"head\", the beginning, of your data.\n                       \n                \n        \n\n\n\ndf.head(10)\n\n\n\n\n\n\n\n\nYear of arrival at port of disembarkation\nVoyage ID\nVessel name\nVoyage itinerary imputed port where began (ptdepimp) place\nVoyage itinerary imputed principal place of slave purchase (mjbyptimp)\nVoyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\nVOYAGEID2\nCaptives arrived at 1st port\nCaptain's name\n\n\n\n\n0\n1714.0\n16109\nFreeke Gally\nBristol\nNaN\nKingston\nNaN\n283.0\nNeale, Alexander\n\n\n1\n1713.0\n16110\nGreyhound Gally\nBristol\nNaN\nJamaica, place unspecified\nNaN\nNaN\nSelkirk, Alexander&lt;br/&gt; Forrest, Henry\n\n\n2\n1714.0\n16111\nJacob\nBristol\nNaN\nKingston\nNaN\n130.0\nNicholls, Philip\n\n\n3\n1714.0\n16112\nJason Gally\nBristol\nNaN\nPort Royal\nNaN\n278.0\nPlummer, John\n\n\n4\n1713.0\n16113\nLawford Gally\nBristol\nAfrica, port unspecified\nNewcastle (Nevis)\nNaN\nNaN\nStretton, Joseph\n\n\n5\n1714.0\n16114\nMercy Gally\nBristol\nAfrica, port unspecified\nBarbados, place unspecified\nNaN\n190.0\nScott, John\n\n\n6\n1714.0\n16115\nMermaid Gally\nBristol\nCape Verde Islands\nKingston\nNaN\n72.0\nBanbury, John&lt;br/&gt; Copinger, James\n\n\n7\n1713.0\n16116\nMorning Star\nBristol\nAfrica, port unspecified\nCharleston\nNaN\nNaN\nPoole, Nicholas\n\n\n8\n1714.0\n16117\nPeterborough\nBristol\nAfrica, port unspecified\nBarbados, place unspecified\nNaN\n200.0\nShawe, John&lt;br/&gt; Martin, Joseph\n\n\n9\n1713.0\n16118\nResolution\nBristol\nGold Coast, port unspecified\nBarbados, place unspecified\nNaN\n255.0\nWilliams, Charles\n\n\n\n\n\n\n\nComparing what we see here with our .csv file it seems that everything went well. We have the data organised in rows and columns. Each column has a name and the table is populated by our data. Some are numbers, some are names and places, some are symbols, some are NaN. We consider our data successfully loaded and ready to be used. It is not time yet to run data analysis, after having loaded the data we need to correctly understand which information contains, then we need to “clean” it, and after that, finally, we can proceed with some data analysis. This is just the beginning, but the best is yet to come!"
  },
  {
    "objectID": "parts/data_analysis/ch2_exploring_data.html",
    "href": "parts/data_analysis/ch2_exploring_data.html",
    "title": "5  Exploring data",
    "section": "",
    "text": "import pandas as pd\ndata_file = 'data/data.csv'\ndf = pd.read_csv(data_file)\n\n\ndf.head(5)\n\n\n\n\n\n\n\n\nYear of arrival at port of disembarkation\nVoyage ID\nVessel name\nVoyage itinerary imputed port where began (ptdepimp) place\nVoyage itinerary imputed principal place of slave purchase (mjbyptimp)\nVoyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\nVOYAGEID2\nCaptives arrived at 1st port\nCaptain's name\n\n\n\n\n0\n1714.0\n16109\nFreeke Gally\nBristol\nNaN\nKingston\nNaN\n283.0\nNeale, Alexander\n\n\n1\n1713.0\n16110\nGreyhound Gally\nBristol\nNaN\nJamaica, place unspecified\nNaN\nNaN\nSelkirk, Alexander&lt;br/&gt; Forrest, Henry\n\n\n2\n1714.0\n16111\nJacob\nBristol\nNaN\nKingston\nNaN\n130.0\nNicholls, Philip\n\n\n3\n1714.0\n16112\nJason Gally\nBristol\nNaN\nPort Royal\nNaN\n278.0\nPlummer, John\n\n\n4\n1713.0\n16113\nLawford Gally\nBristol\nAfrica, port unspecified\nNewcastle (Nevis)\nNaN\nNaN\nStretton, Joseph\n\n\n\n\n\n\n\nNow that we correctly loaded our data in our working environment, it is time to figure out what the data contains. It is always a good idea to look at the dataset documentation (or metadata) to understand where the data comes from, what is the source of all the different records, how data have been collected, and any other possible data related caveat. Diving into documentation is up to you, in this chapter what we want to do is understanding as much as we can from the data itself, looking at its columns, rows, and values.  Every dataset tells a story. You may think about it like a person with a long experience, but not really willing to talk (well, some datasets “talks” more easily than others). It is your role in this case to “interrogate” this person, let him/her to talk, to tell a story and to dive into the details of that story, getting as much information as you can. This also depends on how much do you need to know: are you fine with a simple “chat” or you need the most accurate detailes?  Let’s formulate some questions to begin with: - How big is the data? - What does each row represent? - What does each column represent?\n\n\n \n            \n                    \n                        \n                        \n                            Question(s)\n                        \n                    \n                    \n                        Question: How big is the data?\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Counting the number of rows and columns and checking its size on the disk.\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    Python attribute shape\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        df.shape, the attribute .shape containes the size of the dataframe expressed in rows and columns.     When printed on the screen it will display two numbers, the first one being the number of rows and the second being the number of columns.\n                           \n                \n                \n        \n\n\n\ndf.shape\n\n(36151, 9)\n\n\nOur DataFrame contains data distributed in 36151 rows and 9 columns. It is a quite big dataset. Shall we care how big is our dataset? We should as this may affect our analysis. For example, if we implement a scientific analysis that requires 1 second per row to produce an output, such program would take about 10hrs to analyse the entire dataset, and that is something we should keep in mind. That is why, in general, it is a good idea to test large analysis programs on a small sub-set of data and then, once verified that everything runs smoothly, performing the analysis on the entire dataset.\nLet’s continue exploring our DataFrame. We have 9 columns, we saw them displayed in our notebook and, luckily enough, their names are pretty descriptive, therefore in this case it is quite intuitive to understand what kind of information they contain. It could be useful to store the column names inside a Python variable and to display their names with a corresponding index (this will be useful later).\n\n\n \n            \n                    \n                        \n                        \n                            Task(s)\n                        \n                    \n                    \n                        Display column names with an index\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Identify column names and attributing them an index (starting from zero) depending on their order in the DataFrame (the first column index will be 0, the second 1, and so on).\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    pandas attribute .columns;Python for loop;Python function print().\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        column_names = df.columns, the DataFrame attributes .columns containes the column names of our     DataFrame. We store these names into the variable column_names;print(column_names), we use the function print() to print on the screen the content of the     variable column_names;i=0 \nprint(\"Index ) Column name\") \nfor name in column_names: \n    print(i,\")\",name) \n    i = i + 1 , we first initialise (assign a value) to the Python variable i, this will correspond to the first index.     We then print the string \"Index) Column name\" as a description for what we are going to print later. We finally use a     for loop to scroll the values contained in column_names. What the for loop does     is reading one by one the values stored in column_names and assigning them, one at the time, to the     variable name. It then performs all the instructions \"inside\" the loop (indented text) and it starts     all over again with the next value in column_names until all the values are explored. In our case, inside     the loop we perform just two operations: 1) we both print the current value of the variables i and     name and 2) we increase the value of i by 1. Why do we increase i? Because we     want to display the different column names according to their position in the DataFrame. Our loop automatically updates     the value of the variable name, but it does not increase by one step the index i, so we have     to do it explicitly.\n                           \n                \n                \n        \n            \n                \n                    \n                    \n                        Expert Coding\n                    \n                \n                \n                    \n                        print(\"Index) Column name\") \nfor i,name in enumerate(column_names): \n    print(f\"{i}) {name}\") , you can substitute the previous block of code with this, it performs the same tasks (printing indices and column names) with less lines of code\n                      \n                \n                \n        \n\n\n\ncolumn_names = df.columns\nprint(column_names)\ni=0 \nprint(\"Index ) Column name\") \nfor name in column_names: \n    print(i,\")\",name) \n    i = i + 1 \n\nIndex(['Year of arrival at port of disembarkation', 'Voyage ID', 'Vessel name',\n       'Voyage itinerary imputed port where began (ptdepimp) place',\n       'Voyage itinerary imputed principal place of slave purchase (mjbyptimp) ',\n       'Voyage itinerary imputed principal port of slave disembarkation (mjslptimp) place',\n       'VOYAGEID2', 'Captives arrived at 1st port', 'Captain's name'],\n      dtype='object')\nIndex ) Column name\n0 ) Year of arrival at port of disembarkation\n1 ) Voyage ID\n2 ) Vessel name\n3 ) Voyage itinerary imputed port where began (ptdepimp) place\n4 ) Voyage itinerary imputed principal place of slave purchase (mjbyptimp) \n5 ) Voyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\n6 ) VOYAGEID2\n7 ) Captives arrived at 1st port\n8 ) Captain's name\n\n\nNow we have the column names nicely listed from top to bottom and with their corresponding index assigned to them. You might be tempted to start the indexing from 1, but as in Python the first element of a list (or any other series of elements) has index 0, we also started from zero. You can also obtain the same result with less lines of code, try it out!\n\nprint(\"Index) Column name\") \nfor i,name in enumerate(column_names): \n    print(f\"{i}) {name}\") \n\nIndex) Column name\n0) Year of arrival at port of disembarkation\n1) Voyage ID\n2) Vessel name\n3) Voyage itinerary imputed port where began (ptdepimp) place\n4) Voyage itinerary imputed principal place of slave purchase (mjbyptimp) \n5) Voyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\n6) VOYAGEID2\n7) Captives arrived at 1st port\n8) Captain's name\n\n\nIt is now time to figure out what are the rows about. Looking at the column names, we notice that the second one (index 1) is called “Voyage ID”. This indicates that this column contains a specific identifier for the ship voyage, implying that each row contains specific information about a single trip. To verify that, we need to check if all the values of the Voyage ID column are different, i.e. if they are unique.\n\n\n \n            \n                    \n                        \n                        \n                            Question(s)\n                        \n                    \n                    \n                        Are the values of the Voyage ID column unique?\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Select the column Voyage ID, go through all its 36151 values and check if there are repetitions.\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    pandas column selector method .iloc;pandas attribute .is_unique;Python function print()\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        voyage_id = df.iloc[:,1], we apply the method .iloc[] to select the second column     of our DataFrame and store it in the variable voyage_id. Inside the square brackets of .iloc[]     we can specify rows an columns to select in the form [,]. In this case,     : means that we select ALL the rows, so df.iloc[:,1] selects all the rows of the column     with index 1 (second column)print(voyage_id.is_unique), we print on the screen the pandas attribute .is_unique.     This attribute is True if, indeed, all the values of voyage_id is unique, False otherwise.\n                           \n                \n                \n        \n            \n                \n                    \n                    \n                        Expert Coding\n                    \n                \n                \n                    \n                        print(df.iloc[:,1].is_unique), methods, attributes, and functions can be applied one after another in single line.\n                      \n                \n                \n        \n\n\n\nvoyage_id = df.iloc[:,1]\nprint(voyage_id.is_unique)\n\nTrue\n\n\nWe verified that all the values of the Voyage ID column are unique, this means that all the rows of our DataFrame refer to a single ship voyage. Looking at the other column, we also notice that information where the voyage began, the port where slaves have been purchased, and the port where slaves have been desembarked is provided. This means that for single voyage we can intend the trip from a starting point to a slave purchase port that concludes in the port where slaves have been desembarked. Looking in particular at the fifth column (index 4, “Voyage itinerary imputed principal place of slave purchase”), we notice it contains several NaN. NaN stands for “Not a Number”, it is a value that appears when something goes wrong in one of the processes ran by our program. If something went wrong, why did not our program stop or tell us something about an occuring problem? Because problems may happen more often than you think and if our program stops working everytime it encounters a situation it cannot handle, it would most probably never finish running! In this case, most probably the record does not exists and it has either been indicated by NaN in our original .csv file or read by the pandas method .read_csv() as NaN.  In any case, the presence of NaN or any other missing value can severely affect our data analysis, for this reason before starting analysins the data we need to find and get rid of those value. This process is usually called “data cleaning” and that is exactly what we are going to do in the next chapter, we are going to clean our data."
  },
  {
    "objectID": "parts/data_analysis/ch3_data_cleaning.html",
    "href": "parts/data_analysis/ch3_data_cleaning.html",
    "title": "6  Cleaning data",
    "section": "",
    "text": "import pandas as pd\ndata_file = 'data/data.csv'\ndf = pd.read_csv(data_file)\nprint(df.shape)\n\n(36151, 9)\n\n\n\ncolumn_names = df.columns\ndf.head(5)\n\n\n\n\n\n\n\n\nYear of arrival at port of disembarkation\nVoyage ID\nVessel name\nVoyage itinerary imputed port where began (ptdepimp) place\nVoyage itinerary imputed principal place of slave purchase (mjbyptimp)\nVoyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\nVOYAGEID2\nCaptives arrived at 1st port\nCaptain's name\n\n\n\n\n0\n1714.0\n16109\nFreeke Gally\nBristol\nNaN\nKingston\nNaN\n283.0\nNeale, Alexander\n\n\n1\n1713.0\n16110\nGreyhound Gally\nBristol\nNaN\nJamaica, place unspecified\nNaN\nNaN\nSelkirk, Alexander&lt;br/&gt; Forrest, Henry\n\n\n2\n1714.0\n16111\nJacob\nBristol\nNaN\nKingston\nNaN\n130.0\nNicholls, Philip\n\n\n3\n1714.0\n16112\nJason Gally\nBristol\nNaN\nPort Royal\nNaN\n278.0\nPlummer, John\n\n\n4\n1713.0\n16113\nLawford Gally\nBristol\nAfrica, port unspecified\nNewcastle (Nevis)\nNaN\nNaN\nStretton, Joseph\n\n\n\n\n\n\n\nNow that we got some familiarity with our dataset, it is time to clean our data, i.e. to get rid of all those NaN values and anything else that might effect our data analysis. Where to start? Well, inspecting the DataFrame by eye, we see severl NaN values in the first 5 rows of our DataFrae. The first column we see NaN values is “Voyage itinerary imputed principal place of slave purchase”, the firth column (index 5). It would be nice to check if also other column have NaN values. Let’s start with the first column, “Year of arrival at port of disembarkation” (index 0), let’s check if this column contains any NaN and then we will repeat the same process for all the other columns.\n\n\n \n            \n                    \n                        \n                        \n                            Question(s)\n                        \n                    \n                    \n                        Are there any NaN values in the first column? How many are they?\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Go through all the 36151 entries of the first column and look for NaN values.\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    pandas column selector method .iloc;pandas method .is_na;Python function .sum();Python function print().\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        arr_year = df.iloc[:,0], we first select the first column of the DataFrame using the     pandas method .iloc[] and we store the selected column in the new     variable arr_year (we already used this method in the previous chapter);arr_year_na = arr_year.isna(), we apply the pandas method .isna().     When we apply the .isna() method, we obtain a result with the same dimensions of the     object you applied it to, containing either True or False depending on if the corresponding value was     a NaN (or na, non arithmetic) value or not (indeed the result just answer the question: is this value na?).     We store the result in the variable arr_year_na (you are free to use a more descriptive name);print(arr_year_na.sum()), in Python a True is equivalent to 1 and Fale to 0. This means that     if we have an array (a list or sequence of values) containing True and False, if we sum all the values,     we would obtain the number of True values (as they count as 1 and everything else counts as 0). These     True values correspond to the caseswhen  the method .isna() found a NaN, so that summing     all these values means, as a matter of fact, counting how many NaNs have been found.\n                           \n                \n                \n        \n            \n                \n                    \n                    \n                        Expert Coding\n                    \n                \n                \n                    \n                        print(df.iloc[:,0].isna()), we can apply methods and function one after another, using less coding lines and saving space in our computer memory\n                      \n                \n                \n        \n\n\n\narr_year = df.iloc[:,0]\narr_year_na = arr_year.isna()\nprint(arr_year_na)\nprint('Total number of NaNs in the first column:',arr_year_na.sum())\n\n0        False\n1        False\n2        False\n3        False\n4        False\n         ...  \n36146    False\n36147    False\n36148    False\n36149    False\n36150    False\nName: Year of arrival at port of disembarkation, Length: 36151, dtype: bool\nTotal number of NaNs in the first column: 1\n\n\n\nsolution = 'The first column contains 1 NaN value'\nquestion_box(solution=solution)\n\n\n            \n                \n                    \n                    \n                        Answer\n                    \n                \n                \n                    The first column contains 1 NaN value\n                \n                \n        \n\n\nIn this way we found our that the first column has 1 NaN (or na) value, that would have been quite hard to spot by eye scrolling 36151 lines!  It is great that we found the 1 NaN in the first column, but where exactly is it located? What’s the corresponding Voyage ID of that value?\n\n\n \n            \n                    \n                        \n                        \n                            Question(s)\n                        \n                    \n                    \n                        Where is the NaN value located?\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Go through all the 36151 entries of the first column and look for the NaN value.\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    Python masking.\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        df[arr_year_na], here we will use one of the most useful features when working with     DataFrames: masking. From our previous coding, arr_year_na is an object with the same shape and     features of the first column, but instead of containing years, it contains True and False values,     where True correspond to the NaN value found applying the method .isna(). If we consider     our DataFrame df, we can use this object to select data from it. What are we going to     select? Well, arr_year_na has a name, and that is the name of the first column, so we     will select that. We will also select only the rows where arr_year_na is True.\n                           \n                \n                \n        \n\n\n\ndf[arr_year_na]\n\n\n\n\n\n\n\n\nYear of arrival at port of disembarkation\nVoyage ID\nVessel name\nVoyage itinerary imputed port where began (ptdepimp) place\nVoyage itinerary imputed principal place of slave purchase (mjbyptimp)\nVoyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\nVOYAGEID2\nCaptives arrived at 1st port\nCaptain's name\n\n\n\n\n32248\nNaN\n91909\nKitty\nLiverpool\nNaN\nNaN\nNaN\nNaN\nFisher, Joseph\n\n\n\n\n\n\n\n\n\n\n            \n                \n                    \n                    \n                        Answer\n                    \n                \n                \n                    The row containing a NaN in the first column (\"Year of arrival at port of disembarkation\") has index 32248, it misses information about its itinerary (only the starting port is present) and number of captives, and its ID is 91909\n                \n                \n        \n\n\nIn this way we can inspect the NaN values one by one and we can make a decision about how to handle them. In our DataFrame there are thousands of NaNs (as you will see in a minute) and going through ALL of them one by one is not a good idea. Let’s first try to figure out if the other columns have also NaNs and how many are they. The process will be quite straightforward as we already did it for one of the columns, so what we need to do now is to repeat the same procedure for all the other columns.\n\n\n \n            \n                    \n                        \n                        \n                            Question(s)\n                        \n                    \n                    \n                        Which columns have NaNs? How many are they?\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Go through the DataFrame columns one by one and count the NaNs\n                \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        for column_name in column_names: \n    selected_column = df[column_name] \n    selected_column_na = selected_column.isna() \n    n_nan = selected_column_na.sum() \n    print(column_name,\"has\",n_nan,\"NaN\") , we start with a for loop scrolling all the elements contained in the variable column_names.     In this variable we previously stored all the column names. In this way the variable column_name     will containg a single column name for each iteration of the loop. We then use the column name to select that     specific column in our DataFrame, storing this selected column in the variable selected_column.     We apply the method .isna() to the selected column to obtain, instead of a column of numerical or     string values, a column of True and False (boolean) values. We use the method .sum() to sum all     the True and False values. Keeping in mind that True is equivalent to 1 and False to 0, this will result in     counting all the NaN values in the selected column. We store this last result in the variable n_nan.     We finally print this result, together with the name of the selected column, using the python function print().\n                           \n                \n                \n        \n            \n                \n                    \n                    \n                        Expert Coding\n                    \n                \n                \n                    \n                        for i,column_name in enumerate(column_names): \n        print(f\"{i}) {column_name} has {df[column_name].isna().sum()} NaN\") , we can apply the methods described above one after another, without storing any intermidiate result in a new     variable. We can also use the python function enumerate() to, not only scroll through the df     column names, but also for counting the loop iterations starting from the first one (index 0). The iteration index     will be stored in the variable i\n                      \n                \n                \n        \n\n\n\nfor column_name in column_names:\n    selected_column = df[column_name]\n    selected_column_na = selected_column.isna()\n    n_nan = selected_column_na.sum()\n    print(column_name,'has',n_nan,'NaN')\n\nYear of arrival at port of disembarkation has 1 NaN\nVoyage ID has 0 NaN\nVessel name has 1614 NaN\nVoyage itinerary imputed port where began (ptdepimp) place has 4508 NaN\nVoyage itinerary imputed principal place of slave purchase (mjbyptimp)  has 2210 NaN\nVoyage itinerary imputed principal port of slave disembarkation (mjslptimp) place has 4191 NaN\nVOYAGEID2 has 36101 NaN\nCaptives arrived at 1st port has 17743 NaN\nCaptain's name has 4028 NaN\n\n\nand if we want to keep in mind the column index of each column…\n\nfor i,column_name in enumerate(column_names): \\\n    print(f\"{i}) {column_name} has {df[column_name].isna().sum()} NaN\")\n\n0) Year of arrival at port of disembarkation has 1 NaN\n1) Voyage ID has 0 NaN\n2) Vessel name has 1614 NaN\n3) Voyage itinerary imputed port where began (ptdepimp) place has 4508 NaN\n4) Voyage itinerary imputed principal place of slave purchase (mjbyptimp)  has 2210 NaN\n5) Voyage itinerary imputed principal port of slave disembarkation (mjslptimp) place has 4191 NaN\n6) VOYAGEID2 has 36101 NaN\n7) Captives arrived at 1st port has 17743 NaN\n8) Captain's name has 4028 NaN\n\n\n\n\n\n            \n                \n                    \n                    \n                        Answer\n                    \n                \n                \n                    The only column that does not have NaNs is the Voyage ID column, all the others (excluding the Year of Arrival [...]) have thousands of NaNs.\n                \n                \n        \n\n\nAt this point we have a general idea of the amount of data missing in our DataFrame. The following question is how to deal with this missing data? There are several things we can do, the easiest option would be just exclude it from our DataFrame. However, in order to answer a research question, we often do not need to use or explore ALL the available information and we would usually be interested in some parameters more than others. In this case our data selection could be performed looking at one or more specific columns. What to do with the rest of the NaNs? We can either leave them as they are and trying to figure out how our analysis program will “digest” these values or find good substitute for them. The value of this substitute will depend on the data type of the columns containing the NaN and on our decision. For example the NaN in the columns containing a descriptive string, like the vessel name or the starting port, could be substituted by the string “unknown”. NaNs in the “Captives arrived […]” column could be left as they are (you may be tempted to change them to 0, but zero captives is quite different from unknown number of captives) or substituted by, for example, the average of captives during the same year.  Each choice will have different implications to our final results, the most important thing in this stage is to clearly document our criteria for filtering NaN. In our specific case we will be mostly interested in the data containing the number of captives, so we want to filter our all those rows where the number of captives is NaN. We will then exclude the columns VOYAGEID2 as we already have a voyage ID and it is not listed in the data variable description. To resume, here there are our cleaning criteria: - All the rows not containing data about the number of captives have been removed; - All the NaN values in columns with descriptive information (e.g. names) have been substituted with “unknown”; - The column VOYAGEID2 has been removed from the DataFrame.\n\n\n \n            \n                    \n                        \n                        \n                            Task(s)\n                        \n                    \n                    \n                        Cleaning data\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Remove all the data of the column VOYAGEID2;Going through all the values of the column \"Captives arrived at 1st port\" and remove those rows containing NaN;Going through all the values of the descriptive columns and changing NaN into \"unknown\"Checking how much data have been filtered out by our procedures\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    Python function print();pandas attribute .columns;pandas method drop();pandas method .head();pandas method .dropna();pandas method .fillna();pandas attribute .shape;Python function len();Python operators * (multiplication), / (division), and - (subtraction)\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        print(df.columns), we start printing the name of our DataFrame columns, just as a reminder. We use     the Python function print() and the pandas attribute .columns we already     used several times;column_to_remove = \"VOYAGEID2\" \ncolumn_to_remove_nan = \"Captives arrived at 1st port\" , we store the name of the columns we are interested in into two variables. This is not stricktly     necessary, as we could use the name of the columns directly as input for methods and functions. However, with this     extra step, our code will be more readable as we have chosed two very descriptive variable names: column_to_remove     will, indeed, contain the name of the column we want to remove from our DataFrame (VORAGEID2) and column_to_remove_nan     will contain the name of the column to use as a reference for filtering rows, all the rows haveing NaN in this column     will be removed;cleaned_df_step1 = df.drop(column_to_remove,axis=1), we use the method .drop() to remove     a specific column. In pandas DataFrames each row has an index. This index is different from the index     that every Python list of objects has, i.e. an integer number starting from 0. This DataFrame index is an additional     index that can also be, not only a number, but also a string, or a label. The pandas method .drop()     can be used either to remove columns, depending on column names, or rows, depending on pandas index.     To indicate that we want to target the columns, and not the rows, we need specify the parameter axis equal     to 1 (0 would target the rows). Together with that we need to specify the name of the column we want to remove, stored     in the variable column_to_remove. In this way, we apply the method .drop() to our \"dirty\"     DataFrame df and we store the result into the variable cleaned_df_step1;cleaned_df_step1.head(5), after every filtering operation, it is a good idea to check out the effect on     our output result. In this case we use the pandas method .head() to visualise the first five     rows of our DataFrame cleaned_df_step1cleaned_df_step2 = cleaned_df_step1.dropna(subset=[column_to_remove_nan]), we use the method .dropna()     to filter out all the rows having NaN in a list of column. The list of columns needs to be specified in the function     parameter subset. In this case we have a single column name stored in the variable column_to_remove_nana     and to make it a list we put this variable in between square brackets [ ]. We apply the method to the     DataFrame cleaned_df_step1 and store the result in cleaned_df_step2;cleaned_df = cleaned_df_step2.fillna(\"unknown\"), we finally use the method .fillna() to fill all     the NaN in our DataFrame with a certain value, in this case the string (word) \"unknown\". We apply the method to the     DataFrame cleaned_df_step2 and store the result into cleaned_df;print(cleaned_df.shape) \nn_filtered_rows = len(df)-len(cleaned_df) \nper_cent = (n_filtered_rows/len(df))*100 , this block of Python instructions checks the size of our filtered DataFrame cleaned_df and     compares it with our original, raw, DataFrame df. We first visualise the size of cleaned_df     using the attribute .shape, then we obtain simply the number of rows of each DataFrame using the Python     function len(). We subtract the number of rows of the DataFrames before and after reading to check how many     rows we filtered out and we store the result in the variable n_filtered_rows. We finally compute the percent     of filtered rows compared to the initial size of the DataFrame.\n                           \n                \n                \n        \n\n\n\n# Display the name of the columns first\nprint(df.columns)\n\n# Select our target columns for clearning the data\ncolumn_to_remove = 'VOYAGEID2'\ncolumn_to_remove_nan = 'Captives arrived at 1st port'\n\n# Perform Data Cleaning visualising the result step by step\n# step1, removing column VOYAGEID2 from the DataFrame\ncleaned_df_step1 = df.drop(column_to_remove,axis=1)\ncleaned_df_step1.head(5)\n\nIndex(['Year of arrival at port of disembarkation', 'Voyage ID', 'Vessel name',\n       'Voyage itinerary imputed port where began (ptdepimp) place',\n       'Voyage itinerary imputed principal place of slave purchase (mjbyptimp) ',\n       'Voyage itinerary imputed principal port of slave disembarkation (mjslptimp) place',\n       'VOYAGEID2', 'Captives arrived at 1st port', 'Captain's name'],\n      dtype='object')\n\n\n\n\n\n\n\n\n\nYear of arrival at port of disembarkation\nVoyage ID\nVessel name\nVoyage itinerary imputed port where began (ptdepimp) place\nVoyage itinerary imputed principal place of slave purchase (mjbyptimp)\nVoyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\nCaptives arrived at 1st port\nCaptain's name\n\n\n\n\n0\n1714.0\n16109\nFreeke Gally\nBristol\nNaN\nKingston\n283.0\nNeale, Alexander\n\n\n1\n1713.0\n16110\nGreyhound Gally\nBristol\nNaN\nJamaica, place unspecified\nNaN\nSelkirk, Alexander&lt;br/&gt; Forrest, Henry\n\n\n2\n1714.0\n16111\nJacob\nBristol\nNaN\nKingston\n130.0\nNicholls, Philip\n\n\n3\n1714.0\n16112\nJason Gally\nBristol\nNaN\nPort Royal\n278.0\nPlummer, John\n\n\n4\n1713.0\n16113\nLawford Gally\nBristol\nAfrica, port unspecified\nNewcastle (Nevis)\nNaN\nStretton, Joseph\n\n\n\n\n\n\n\n\n# step2, removing all the rows haveing NaN in the \"Captives arrived at 1st port\" column\ncleaned_df_step2 = cleaned_df_step1.dropna(subset=[column_to_remove_nan])\ncleaned_df_step2.head(5)\n\n\n\n\n\n\n\n\nYear of arrival at port of disembarkation\nVoyage ID\nVessel name\nVoyage itinerary imputed port where began (ptdepimp) place\nVoyage itinerary imputed principal place of slave purchase (mjbyptimp)\nVoyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\nCaptives arrived at 1st port\nCaptain's name\n\n\n\n\n0\n1714.0\n16109\nFreeke Gally\nBristol\nNaN\nKingston\n283.0\nNeale, Alexander\n\n\n2\n1714.0\n16111\nJacob\nBristol\nNaN\nKingston\n130.0\nNicholls, Philip\n\n\n3\n1714.0\n16112\nJason Gally\nBristol\nNaN\nPort Royal\n278.0\nPlummer, John\n\n\n5\n1714.0\n16114\nMercy Gally\nBristol\nAfrica, port unspecified\nBarbados, place unspecified\n190.0\nScott, John\n\n\n6\n1714.0\n16115\nMermaid Gally\nBristol\nCape Verde Islands\nKingston\n72.0\nBanbury, John&lt;br/&gt; Copinger, James\n\n\n\n\n\n\n\n\n# step3, changing all the other NaN into unknown\ncleaned_df = cleaned_df_step2.fillna(\"unknown\")\ncleaned_df.head(5)\n\n\n\n\n\n\n\n\nYear of arrival at port of disembarkation\nVoyage ID\nVessel name\nVoyage itinerary imputed port where began (ptdepimp) place\nVoyage itinerary imputed principal place of slave purchase (mjbyptimp)\nVoyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\nCaptives arrived at 1st port\nCaptain's name\n\n\n\n\n0\n1714.0\n16109\nFreeke Gally\nBristol\nunknown\nKingston\n283.0\nNeale, Alexander\n\n\n2\n1714.0\n16111\nJacob\nBristol\nunknown\nKingston\n130.0\nNicholls, Philip\n\n\n3\n1714.0\n16112\nJason Gally\nBristol\nunknown\nPort Royal\n278.0\nPlummer, John\n\n\n5\n1714.0\n16114\nMercy Gally\nBristol\nAfrica, port unspecified\nBarbados, place unspecified\n190.0\nScott, John\n\n\n6\n1714.0\n16115\nMermaid Gally\nBristol\nCape Verde Islands\nKingston\n72.0\nBanbury, John&lt;br/&gt; Copinger, James\n\n\n\n\n\n\n\n\n# step4, checking how much data we filtered out\nprint(cleaned_df.shape)\nn_filtered_rows = len(df)-len(cleaned_df)\nper_cent = (n_filtered_rows/len(df))*100\nprint('We filtered out: ',len(df)-len(cleaned_df),', corresponding to about', round(per_cent), '% of our initial data')\n\n(18408, 8)\nWe filtered out:  17743 , corresponding to about 49 % of our initial data\n\n\nIt seems that because of our filtering, almost half of our data will be excluded from the analysis. This is a quite large percent and we may decide to re-think our filtering criteria to include more data. For example, we could substitue the missing value in the Captives column with an avarage number of captived per trip. For the purpose of our workshop, we will keep the current filtering criteria and keep our filtered DataFrame as it is.\nAt this point we obtained a “clean” DataFrame, cleaned_df, containing 18408 rows with values organised in 8 columns. We can now start diving deep in the analysis of our DataFrame, we are ready to interrogate this dataset and see which kind of story it is going to tell us."
  },
  {
    "objectID": "parts/data_analysis/ch4_data_analysis.html",
    "href": "parts/data_analysis/ch4_data_analysis.html",
    "title": "7  Analysing data",
    "section": "",
    "text": "8 Data Analysis\nIt is finally time to ask questions to our data. Let’s start with some simple ones regaring the time span of our dataset.\nQuestion(s)\n                        \n                    \n                    \n                        What's the year of the very first and last voyage record?How many years does our data set span?How does that compare to unfiltered data?\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Go through the \"Year of arrival at port of disembarkation\" column and look for the largest and smallest number;Repeat the same procedure for the raw data set and compare.\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    pandas method .iloc();Python functions min() and max();Python print() function.\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        arrival_year = cleaned_df.iloc[:,0], we use the method .iloc[] to select the first column     of the DataFrame cleaned_df and we store the column in the variable arrival_year;first_year = min(arrival_year)\nlast_year = max(arrival_year)\nyear_span = last_year-first_year, we use the Python functions min() and max() to compute the minimum and     maximum values of the year column storing their values in first_year and last_year,     respectively. We also compute the difference between these two values and store the result in year_span.\narrival_year = cleaned_df.iloc[:,0]\nfirst_year = min(arrival_year)\nlast_year = max(arrival_year)\nyear_span = last_year-first_year\n\nprint(first_year)\nprint(last_year)\nprint(year_span)\n\n1520.0\n1866.0\n346.0\narrival_year_raw = df.iloc[:,0]\nfirst_year_raw = min(arrival_year_raw)\nlast_year_raw = max(arrival_year_raw)\nyear_span_raw = last_year_raw-first_year_raw\n\nprint(first_year_raw)\nprint(last_year_raw)\nprint(year_span_raw)\n\n1514.0\n1866.0\n352.0\nAnswer\n                    \n                \n                \n                    Our cleaned data spans from 1520 to 1866, so 346. Comparing these numbers with our original, uncleaned, DataFrame, we can notice that the filtered data does not include the period between 1514 and 1520, so 6 years.\nWe can keep asking questions about numerical values. We focused on time in our last question, let’s focus on the number of captives this time.\nQuestion(s)\n                        \n                    \n                    \n                        How many captives have been disembarked between 1520 and 1866?How many captives have been disembarked on average per voyage?How many captives have been disembarked on average per year?How do these numbers change looking at the raw data?\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Go through the \"Captives arrived at 1st port\" column and compute the total number of captives;Divide the previous result by the number of voyages;Divide the total number of captives by the number of years;Adjust the previous estimates considering the average number of captives per voyage and the number of filtered rows.\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    pandas method .iloc();Python function sum();Python function len();Python function round();Python function print() .\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        n_captives = cleaned_df.iloc[:,6], we apply the method .iloc[] to select the     \"Captives arrived at 1st port\" column and we store the column in the variable n_captives;tot_captives = sum(n_captives), we use the Python function sum() to compute the sum     of all the values in n_captives and we stored the result in the variable tot_captivesave_cap_per_voyage = tot_captives/len(cleaned_df)\nave_cap_per_year = tot_captives/year_span, we use the total number of captives just computed to calculate the average number of captives     per vorage (dividing it by the number of rows) and per year (dividing it by the year span). We store the result     in ave_cap_per_voyage and ave_cap_per_year, respectively;filtered_rows = len(df)-len(cleaned_df)\ntot_captives_ext = tot_captives + ave_cap_per_voyage*filtered_rows\nave_cap_per_year = tot_captives/year_span, we compute the number of filtered rows subtracting the length of our original DataFrame df     by the length of our cleaned DataFrame cleaned_df. Multiplying this number by the average number of     captives per voyage (remember that in our DataFrames each row corresponds to a voyage), we obtain an estimate of the     number of captives that have not been considered as the corresponding data is missing and, as such, it has been filtered     out from our analysis. We store this estimate in the variable tot_captives_ext. We then adjust our estimate     of the number of captives per years using this value and dividing it by year_span_raw, the year span of     our original DataFrame. We store this value into the variable ave_cap_per_year_adj;print(round()), we print different results related to people using the Python function     round() that, without any other argument, approximate decimal numbers to the closest integer.\nn_captives = cleaned_df.iloc[:,6]\ntot_captives = sum(n_captives)\nave_cap_per_voyage = tot_captives/len(cleaned_df)\nave_cap_per_year = tot_captives/year_span\nprint('Total n. of captives:',tot_captives)\nprint('Average captives per voyage',round(ave_cap_per_voyage))\nprint('Average captives per year',round(ave_cap_per_year))\n\nTotal n. of captives: 5082756.0\nAverage captives per voyage 276\nAverage captives per year 14690\nfiltered_rows = len(df)-len(cleaned_df)\ntot_captives_ext = tot_captives + ave_cap_per_voyage*filtered_rows\nave_cap_per_year_adj = tot_captives_ext/year_span_raw\nprint('Extimated total n. of captives',round(tot_captives_ext))\nprint('Adjusted average captives per year', round(ave_cap_per_year_adj))\n\nExtimated total n. of captives 9981894\nAdjusted average captives per year 28358\nAnswer\n                    \n                \n                \n                    Our first estimate of the total number of captives was 5 082 756 in the time period between 1520 and 1866,  and a corresponding average of captives per year of 14 690. However, this estimate did not take into account filtered data. In our data filtering we excluded many rows, so many voyages. Almost 50% of the initial voyages has been filtered our. We therefore computed the average of captives per voyage and estimated the number of filtered out captives. The result is a number that almost double (as expected) our previous estimate: 9 981 894 people, with an average of 28 358 people traded per year.\nSo far we computed numbers, but data can be most effectively described using visualization. In our DataFrame we have information about three different locations: the place where the voyage started, the principal port of slave purchase, and the principal port of slave disembarkation. Let’s have a closer look at these locations.\nQuestion(s)\n                        \n                    \n                    \n                        Which is the town where most of the voyages started?Can we sort towns according to the number of voyages that started there?How about the other two locations, place of slave purchase and disembarkation?\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Go through the \"Voyage itinerary imputed port where began (ptdepimp) place\" column and count how many times     a town name occurs;Repeat this procedure for all the towns;Sort towns according to how many times their names show up;Make a plot with town names on the vertical axis and how many times they have been starting point for voyages in     the orizontal axis;Repeat the same procedure for the other two locations: port of slave purchase and disembarkation.\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    pandas method .iloc[]pandas method .value_counts()Python package seaborn;Python package matplotlib.pyplot;matplotlib.pyplot function .subplots();seaborn function .barplot()\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        start_port = cleaned_df.iloc[:,3], we use the pandas method .iloc[] to     select our target column. In this case it is \"Voyage itinerary imputed port where began (ptdepimp) place\", the     fourth column of our DataFrame (Python index 3). We store this column in the variable start_port;start_port_counts = start_port.value_counts(), we want to count how many times a certain town name     occurs in our column and the pandas method .value_counts() does exactly this. After     applying the method, we obtain a pandas DataFrame with a single column (a Series) having as index the     name of the town and as value the number of times that town shows up in the column start_port. We store     this result in start_port_counts;start_port_counts, we display the content of the variable start_port_counts on the screen;import seaborn as sns, we import the Python package seaborn and we give it the alias     sns. This means that, from now own, if we want to use a certain function belonging to this package, we     will call it using: sns.function_name();import matplotlib.pyplot as plt, we import the Python package matplotlib.pyplot and we     give it the alias plt;fig, new_ax = plt.subplots(nrows=1,ncols=1,figsize=(8,8)), we use the matplotlib.pyplot     function .subplots() to initialise an empty plot. .subplots() it is generally used to create     a grid of plots and its first two arguments (nrows and ncols) specify the number of rows     and columns in this grid. In our case, we just want a single plot, therefore we specify a grid having only one column     and one row. The parameter figsize specifies the size of the figure in inches. In our case     ((8, 8)) it will be 8 inches wide and 8 inches high. This function returns two different objects: a     Figure and an Axes. The Figure is the white space where we are going to put our plot, it is our plot container. The     Axes, despite its name, it is the plot itself. We did not initialise our plot yet, so the Axes object is empty, but this is     one of the standard procedure in Python: first create the space where to plot, then plot the data. Figure and Axes are     stored in the variables fig and new_ax, respectively;filter = start_port_counts &gt; 50, if we plot ALL our data, the plot is going to look overcrowded. For this     reason, we can specify a threeshold and plotting only towns that appear a number of times that is higher than our     threeshold. In our case, and in the specific case of this plot, this number will be 50.     start_port_counts &gt; 50 is a Python condition, it basically creates an array with the same size     and features of start_port_counts containing boolean values (True and False) instead of numerical values.     When True and when False? If a value in start_port_counts is higher than 50, the corresponding boolean     value will be True, otherwise it will be False. We store this boolean array in the variable filter as     this can be used as a mask to filter out only those values corresponding to True     from an array with its same dimention and features;sns.barplot(ax=new_ax,x=start_port_counts[filter],y=start_port_counts.index[filter]), we use the     seaborn function .barplot() to create (guess what??) a barplot. The barplot will be     located in our just created Axes object, this is implemented using the argument ax=new_ax. We need to     specify which data to plot in the horizontal x and y axis. We want to plot town name in the     y axis and town name counts in the y axis. We also want to display only those data with more than 50 counts. We can     obtain this with x=start_port_counts[filter] and y=start_port_counts.index[filter], as the     town name is specified in the start_port_counts index and the town counts in the     start_port_counts values;We repeat the same procedure for the fifth and sixth column of our DataFrame.\nstart_port = cleaned_df.iloc[:,3]\nstart_port_counts = start_port.value_counts()\nprint(type(start_port_counts))\nstart_port_counts\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\nVoyage itinerary imputed port where began (ptdepimp) place\nLiverpool                       3227\nunknown                         2005\nLondon                          1874\nBahia, place unspecified        1815\nRio de Janeiro                  1464\n                                ... \nMangaratiba                        1\nMediterranean coast (France)       1\nCanasí                             1\nSanta Catarina                     1\nPortland                           1\nName: count, Length: 176, dtype: int64\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, new_ax = plt.subplots(nrows=1,ncols=1,figsize=(8,8))\nfilter = start_port_counts &gt; 50\nsns.barplot(ax=new_ax,x=start_port_counts[filter],y=start_port_counts.index[filter])\n\n&lt;Axes: xlabel='count', ylabel='Voyage itinerary imputed port where began (ptdepimp) place'&gt;\nmain_pur_port = cleaned_df.iloc[:,4]\nmain_pur_counts = main_pur_port.value_counts()\nmain_pur_counts\n\nVoyage itinerary imputed principal place of slave purchase (mjbyptimp) \nAfrica, port unspecified                                3177\nLuanda                                                  1447\nWest Central Africa and St. Helena, port unspecified    1139\nBonny                                                    853\nGold Coast, port unspecified                             787\n                                                        ... \nPetit Mesurado                                             1\nEva                                                        1\nPokesoe (Princes Town)                                     1\nSassandra                                                  1\nSugary (Siekere)                                           1\nName: count, Length: 161, dtype: int64\nfig, ax = plt.subplots(1,1,figsize=(8,8))\nfilter = main_pur_counts &gt; 200\nsns.barplot(ax=ax,x=main_pur_counts[filter],y=main_pur_counts.index[filter])\n\n&lt;Axes: xlabel='count', ylabel='Voyage itinerary imputed principal place of slave purchase (mjbyptimp) '&gt;\nmain_dis_port = cleaned_df.iloc[:,5]\nmain_dis_counts = main_dis_port.value_counts()\nmain_dis_counts\n\nVoyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\nBahia, place unspecified       1720\nRio de Janeiro                 1651\nKingston                       1576\nBarbados, place unspecified    1524\nHavana                          893\n                               ... \nFrance, place unspecified         1\nSanta Marta                       1\nDois Rios                         1\nMaceió                            1\nBonny                             1\nName: count, Length: 240, dtype: int64\nfig, ax = plt.subplots(1,1,figsize=(8,8))\nfilter = main_dis_counts &gt; 150\nsns.barplot(ax=ax,x=main_dis_counts[filter],y=main_dis_counts.index[filter])\n\n&lt;Axes: xlabel='count', ylabel='Voyage itinerary imputed principal port of slave disembarkation (mjslptimp) place'&gt;\nAnswer\n                    \n                \n                \n                    The ports where most of the voyages took place are Liverpool and London (with a considerable percent being unkown). Most of the places where slaves have been purchased remain unkown, the second most occuring purchanse place is Luanda (mid Africa). The most frequent places of disimbarkation are Bahia and Rio de Janeiro, on the opposite side of the Atlantic Ocean at a similar latitude.\nLet’s try to make now a different type of visualization, a time series, i.e. a plot where we see how parameters change over time\nQuestion(s)\n                        \n                    \n                    \n                        How does the total number of disembarkated captives change over time?\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Consider the data set columns containing information about time and number of captivesGroup data rows by yearSum the number of captives per yearMake a plot with years (time) on the x axis and number of captives on the y axisLocate the minimum and the maximum of captives per year\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    pandas method .groupby();parnas method .mean();Python package matplotlib.pyplot;Python package seaborn;matplotlib.pyplot function .subplots();seaborn function .lineplot();matplotlib.pyplot function .grid();pandas method .idxmin();pandas method .idxmax();Python function print()\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        col_to_group = \"Year of arrival at port of disembarkation\"\ncol_to_sum = \"Captives arrived at 1st port\", we select the two columns in the DataFrame we are interested in, the one containing the time     information and the one containing the number of captives. We store the name of these two columns in the     variables col_to_group and col_to_sum, respectively;df_per_year = cleaned_df.groupby(col_to_group)[col_to_average].sum(), we first apply the     method .groupby() to our cleaned DataFrame. .groupby() does exactly what it says, it     groups data according to a certain column (in our case the col_to_group column), this means that any     other method we are going to apply from now own, it will operate on groups of rows instead of the entire DataFrame.     For example, if we apply a method perfoming summation or average on a DataFrame grouped by year, python will     compute the sum or the average on those groups of data. In a DataFrame there are many columns containing values     to sum and average, so which column is Python going to choose? We need to specify it, in our case we do that     with [col_to_sum], where the column to sum contains information about the number of captives. We     finally apply the method .sum() to compute the sum of number of captives on DataFrame rows grouped     by year. We store the result in the variable df_per_year;sum_df_per_year, we display our last result on the screen;fig, my_ax = plt.subplots(nrows=1,ncols=1,figsize=(8,8)), we use the function .subplots()     to create a single squared empty plot with size 8 inches. We obtain a Figure object (the container of our plot) and     a Axes object (an empty plot). We store these two objects in the variables fig and my_ax;sns.lineplot(ax=my_ax,x=sum_df_per_year.index,y=sum_df_per_year), we use the method     .lineplot() to create a line plot (data points connected by a line). We plot the index of the Series     (a Series is a DataFrame with only one column) df_per_year on the x axis and its values on the y axis.plt.grid(), we use the function .grid() to overplot a grid on our plot, with the purpose of     better describe the data by eye;max_index = df_per_year.idxmax();\nmin_index = df_per_year.idxmin();\nprint('The minimum number of captives per year is:', df_per_year[min_index],'on',min_index);\nprint('The maximum number of captives per year is:', df_per_year[max_index],'on',max_index)., we apply the method .idxmax() to the Series df_per_year to get the index     corresponding to the maximum of its values. Remember that df_per_year indeces are year and its values     are the sum of number of captives in that year. Therefore, obtaining the index corresponding to the maximum value     means determining the year having the maximum number of captives. We store this result in the variable     max_index. We can use this index to retrieve the corresponding value:     df_per_year[max_index]. We repeat the same procedure for the minimum value using     .idxmin(). We finally print both results on the screen with the function print().\ncol_to_group = 'Year of arrival at port of disembarkation'\ncol_to_sum = 'Captives arrived at 1st port'\ndf_per_year = cleaned_df.groupby(col_to_group)[col_to_sum].sum()\nprint(df_per_year.shape)\ndf_per_year\n\n(298,)\n\n\nYear of arrival at port of disembarkation\n1520.0       44.0\n1526.0      115.0\n1527.0       46.0\n1532.0      589.0\n1534.0      354.0\n           ...   \n1862.0    11407.0\n1863.0     6739.0\n1864.0     3298.0\n1865.0      795.0\n1866.0      700.0\nName: Captives arrived at 1st port, Length: 298, dtype: float64\nfig, ax = plt.subplots(1,1,figsize=(8,8))\nsns.lineplot(ax=ax,x=df_per_year.index,y=df_per_year)\nplt.grid()\n\n/Users/xizg0003/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/xizg0003/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\nmax_index = df_per_year.idxmax()\nmin_index = df_per_year.idxmin()\nprint('The minimum number of captives per year is:', df_per_year[min_index],'on',min_index)\nprint('The maximum number of captives per year is:', df_per_year[max_index],'on',max_index)\n\nThe minimum number of captives per year is: 2.0 on 1538.0\nThe maximum number of captives per year is: 79472.0 on 1829.0\nsolution = 'The total number of captives is almost constant up to 1650, with the exception of two peaks around 1600 \\\nand 1622. The number increases steadily up to 1800 and decreases afterwords. The times series is not smooth, but it proceeds \\\nby low and high peek. The number of captives per year reaches its maximum on 1829 with almost 80 thousands slaves traded \\\nthat year. The minimum is 2 captives on 1538.'\nquestion_box(solution=solution)\n\n\n            \n                \n                    \n                    \n                        Answer\n                    \n                \n                \n                    The total number of captives is almost constant up to 1650, with the exception of two peaks around 1600 and 1622. The number increases steadily up to 1800 and decreases afterwords. The times series is not smooth, but it proceeds by low and high peek. The number of captives per year reaches its maximum on 1829 with almost 80 thousands slaves traded that year. The minimum is 2 captives on 1538.\nTime series are very interesting to describe the trends of phenomema at different scale. Our plot ticks are separated by 50 years, this is fine to visualise trends over centuries, but we cannot see what’s happening on decades.\nQuestion(s)\n                        \n                    \n                    \n                        What's the disembarked captive trend between 1700 and 1750?\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Go through the steps for making the previous plot filtering data spanning between 1700 and 1750\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    Python package matplotlib.pyplot;Python package seaborn;matplotlib.pyplot function .subplots();seaborn function .lineplot();matplotlib.pyplot function .grid().\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        time_filter = (df_per_year.index &gt; 1700) & (df_per_year.index &lt; 1750), we start using the pandas Series df_per_year. This     series contains information about the total number of captives per year. Years are contained in its index, number of captives in its values. We want to select     data between 1700 and 1750, so with years larger than 1700 and smaller than 1750. We do that using two Python conditions: df_per_year.index &gt; 1700 and     df_per_year.index &lt; 1750. As we want these two conditions to be true at the same time, we specify both of them with the symbol & in     between. In Python, the commertial & indicates a logical AND, this means that the condition before and after the AND must both be simultaneously true. We store     this condition into the variable time_filter;fig, ax = plt.subplots(1,1,figsize=(8,8))\nsns.lineplot(ax=ax,x=df_per_year.index[time_filter],y=df_per_year[time_filter])\nplt.grid(), as we did in the previous task, we plot total number of captives versus years. This time, instead of using our full array, we want to plot only     a certain time range. This has been specified in the condition stored into time_filter. Therefore, compared to our previous plot, we are going to     user df_per_year.index[time_filter] and df_per_year[time_filter] instead of df_per_year.index and df_per_year, respectively.\ntime_filter = (df_per_year.index &gt; 1700) & (df_per_year.index &lt; 1750)\nfig, ax = plt.subplots(1,1,figsize=(8,8))\nsns.lineplot(ax=ax,x=df_per_year.index[time_filter],y=df_per_year[time_filter])\nplt.grid()\n\n/Users/xizg0003/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/xizg0003/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):"
  },
  {
    "objectID": "parts/data_analysis/ch2_exploring_data.html#previous-steps",
    "href": "parts/data_analysis/ch2_exploring_data.html#previous-steps",
    "title": "5  Exploring data",
    "section": "Previous steps",
    "text": "Previous steps\n\nimport pandas as pd\ndata_file = 'data/data.csv'\ndf = pd.read_csv(data_file)\n\n\ndf.head(5)\n\n\n\n\n\n\n\n\nYear of arrival at port of disembarkation\nVoyage ID\nVessel name\nVoyage itinerary imputed port where began (ptdepimp) place\nVoyage itinerary imputed principal place of slave purchase (mjbyptimp)\nVoyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\nVOYAGEID2\nCaptives arrived at 1st port\nCaptain's name\n\n\n\n\n0\n1714.0\n16109\nFreeke Gally\nBristol\nNaN\nKingston\nNaN\n283.0\nNeale, Alexander\n\n\n1\n1713.0\n16110\nGreyhound Gally\nBristol\nNaN\nJamaica, place unspecified\nNaN\nNaN\nSelkirk, Alexander&lt;br/&gt; Forrest, Henry\n\n\n2\n1714.0\n16111\nJacob\nBristol\nNaN\nKingston\nNaN\n130.0\nNicholls, Philip\n\n\n3\n1714.0\n16112\nJason Gally\nBristol\nNaN\nPort Royal\nNaN\n278.0\nPlummer, John\n\n\n4\n1713.0\n16113\nLawford Gally\nBristol\nAfrica, port unspecified\nNewcastle (Nevis)\nNaN\nNaN\nStretton, Joseph\n\n\n\n\n\n\n\nNow that we correctly loaded our data in our working environment, it is time to figure out what the data contains. It is always a good idea to look at the dataset documentation (or metadata) to understand where the data comes from, what is the source of all the different records, how data has been collected, and any other possible data related caveat. Diving into the data documentation is up to you, in this chapter what we want to do is understanding as much as we can from the data itself, looking at its columns, rows, and values.  Every dataset tells a story. You may think about it like a person with a long experience, but not really willing to talk (well, some datasets “talk” more easily than others). It is your role in this case to “interrogate” the data, let it to talk, to tell a story and to dive into the details of that story, getting as much information as you can. This also depends on how much you need to know: will you be satisfied by a small “chat” or you need to know all kind of details?  Let’s formulate some questions to begin with.\n\n\n \n            \n                    \n                        \n                        \n                            Question(s)\n                        \n                    \n                    \n                        Question: How big is the data?\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Counting the number of rows and columns and checking its size on the disk.\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    pandas attribute shape\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        df.shape, the attribute .shape contains the size of the DataFrame expressed in rows and     columns. When printed on the screen it will display two numbers, number of rows and number of columns.\n                           \n                \n                \n        \n\n\n\ndf.shape\n\n(36151, 9)\n\n\n\nsolution = 'Our DataFrame contains data distributed in 36151 rows and 9 columns. '\nquestion_box(solution=solution)\n\n\n            \n                \n                    \n                    \n                        Answer\n                    \n                \n                \n                    Our DataFrame contains data distributed in 36151 rows and 9 columns. \n                \n                \n        \n\n\nIt is a quite big dataset. Shall we care about how big is our dataset? We should as this may affect our analysis. For example, if we implement a scientific analysis that requires 1 second per row to produce an output, such program would take about 10hrs to analyse the entire dataset, and that is something we should keep in mind. That is why, in general, it is a good idea to test large analysis programs on a small sub-set of data and then, once verified that everything runs smoothly, to perform the analysis on the entire dataset.\nLet’s continue exploring our DataFrame. We have 9 columns, we saw them displayed in our notebook and, luckily enough, their names are pretty descriptive, therefore, in this case, it is quite intuitive to understand what kind of information they contain. It could be useful to store the column names inside a Python variable and to display their names with a corresponding index (this will be useful later).\n\n\n \n            \n                    \n                        \n                        \n                            Task(s)\n                        \n                    \n                    \n                        Display the DataFrame column names with an index\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Identify column names and assign them an index (starting from zero) depending on their order in the DataFrame (the first column index will be 0, the second 1, and so on).\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    pandas attribute .columns;Python for loop;Python function print().\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        column_names = df.columns, the DataFrame attribute .columns contains the column names of our     DataFrame. We store these names into the variable column_names;print(column_names), we use the function print() to print on the screen the content of the     variable column_names;i=0 \nprint(\"Index ) Column name\") \nfor name in column_names: \n    print(i,\")\",name) \n    i = i + 1 , we first initialise (assign a value) to the Python variable i, this will correspond to the first index.     We then print the string \"Index) Column name\" as a description for what we are going to print later. We finally use a     for loop to scroll the values contained in column_names. What the for loop does     is reading one by one the values stored in column_names and assigning them, one at the time, to the     variable name. It then performs all the instructions \"inside\" the loop (indented text) and, once all the     instructions are executed, it starts     all over again with the next value in column_names until all the values are explored. In our case, inside     the loop we perform just two operations: 1) we both print the current value of the variables i and     name and 2) we increase the value of i by 1. Why do we increase i? Because we     want to display the different column names according to their position in the DataFrame. Our loop automatically updates     the value of the variable name, but it does not increase by one step the index i, so we have     to do it explicitly.\n                           \n                \n                \n        \n            \n                \n                    \n                    \n                        Expert Coding\n                    \n                \n                \n                    \n                        print(\"Index) Column name\") \nfor i,name in enumerate(column_names): \n    print(f\"{i}) {name}\") , you can substitute the previous block of code with this, it performs the same tasks (printing indices and column names) with less lines of code\n                      \n                \n                \n        \n\n\n\ncolumn_names = df.columns\nprint(column_names)\ni=0 \nprint(\"Index ) Column name\") \nfor name in column_names: \n    print(i,\")\",name) \n    i = i + 1 \n\nIndex(['Year of arrival at port of disembarkation', 'Voyage ID', 'Vessel name',\n       'Voyage itinerary imputed port where began (ptdepimp) place',\n       'Voyage itinerary imputed principal place of slave purchase (mjbyptimp) ',\n       'Voyage itinerary imputed principal port of slave disembarkation (mjslptimp) place',\n       'VOYAGEID2', 'Captives arrived at 1st port', 'Captain's name'],\n      dtype='object')\nIndex ) Column name\n0 ) Year of arrival at port of disembarkation\n1 ) Voyage ID\n2 ) Vessel name\n3 ) Voyage itinerary imputed port where began (ptdepimp) place\n4 ) Voyage itinerary imputed principal place of slave purchase (mjbyptimp) \n5 ) Voyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\n6 ) VOYAGEID2\n7 ) Captives arrived at 1st port\n8 ) Captain's name\n\n\nNow we have the column names nicely listed from top to bottom and with their corresponding index assigned to them. You might be tempted to start the indexing from 1, but as in Python the first element of a list (or any other series of elements) has index 0, we started counting from zero. You can obtain the same result with less lines of code, try it out!\n\nprint(\"Index) Column name\") \nfor i,name in enumerate(column_names): \n    print(f\"{i}) {name}\") \n\nIndex) Column name\n0) Year of arrival at port of disembarkation\n1) Voyage ID\n2) Vessel name\n3) Voyage itinerary imputed port where began (ptdepimp) place\n4) Voyage itinerary imputed principal place of slave purchase (mjbyptimp) \n5) Voyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\n6) VOYAGEID2\n7) Captives arrived at 1st port\n8) Captain's name\n\n\nIt is now time to figure out what are the rows about. Looking at the column names, we notice that the second one (index 1) is called “Voyage ID”. This indicates that this column contains a specific identifier for the ship voyage, implying that each row contains specific information about a single trip. To verify that each row corresponds to a single voyage, we need to check if all the values of the Voyage ID column are different, i.e. if they are unique.\n\n\n \n            \n                    \n                        \n                        \n                            Question(s)\n                        \n                    \n                    \n                        Are the values of the Voyage ID column unique?\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Select the column Voyage ID, go through all its 36151 values and check if there are repetitions.\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    pandas column selector method .iloc[];pandas attribute .is_unique;Python function print()\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        voyage_id = df.iloc[:,1], we apply the method .iloc[] to select the second column     of our DataFrame and store it in the variable voyage_id. Inside the square brackets of .iloc[]     we can specify rows an columns to select in the form [selected_rows,selected_columns]. In this case,     : means that we select ALL the rows, so df.iloc[:,1] selects all the rows of the column     with index 1 (second column)print(voyage_id.is_unique), we print on the screen the pandas attribute .is_unique.     This attribute is True if, indeed, all the values of voyage_id are unique, False otherwise.\n                           \n                \n                \n        \n            \n                \n                    \n                    \n                        Expert Coding\n                    \n                \n                \n                    \n                        print(df.iloc[:,1].is_unique), methods, attributes, and functions can be applied one after another     in a single line of code.\n                      \n                \n                \n        \n\n\n\nvoyage_id = df.iloc[:,1]\nprint(voyage_id.is_unique)\n\nTrue\n\n\nWe verified that all the values of the Voyage ID column are unique, this means that each of the rows of our DataFrame refers to a single ship voyage. Looking at the other columns, we also notice that information where the voyage began, the port where slaves have been purchased, and the port where slaves have been desembarked is provided. Looking in particular at the fifth column (index 4, “Voyage itinerary imputed principal place of slave purchase”), we notice it contains several NaNs. NaN stands for “Not a Number”, it is a value that appears when something goes wrong in one of the processes ran by our program. If something went wrong, why did not our program stop or told us something about an occuring problem? Because problems may happen more often than you think and if our program stops working everytime it encounters a situation it cannot handle, it would most probably never finish running! In this case, most probably the record does not exist so the data set cell has been filled by NaN, either in our original .csv file or by the pandas method .read_csv(). NaN are not necesseraly something bad, as they can be easily identified and eventually corrected (or simply ignored). Incorrect or missing data may be much harder to spot and correct.  In any case, the presence of NaNs or any other missing value can severely affect our data analysis, for this reason before starting analysing the data we need to find and get rid of those values. This process is usually called “data cleaning” and that is exactly what we are going to do in the next chapter."
  },
  {
    "objectID": "parts/data_analysis/ch4_data_analysis.html#previous-steps",
    "href": "parts/data_analysis/ch4_data_analysis.html#previous-steps",
    "title": "7  Analysing data",
    "section": "Previous steps",
    "text": "Previous steps\n\nimport pandas as pd\ndata_file = 'data/data.csv'\ndf = pd.read_csv(data_file)\ncleaned_df = df.drop('VOYAGEID2',axis=1).dropna(subset=['Captives arrived at 1st port']).fillna(\"unknown\")\ncleaned_col_names = cleaned_df.columns\ncleaned_df.head(10)\n\n\n\n\n\n\n\n\nYear of arrival at port of disembarkation\nVoyage ID\nVessel name\nVoyage itinerary imputed port where began (ptdepimp) place\nVoyage itinerary imputed principal place of slave purchase (mjbyptimp)\nVoyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\nCaptives arrived at 1st port\nCaptain's name\n\n\n\n\n0\n1714.0\n16109\nFreeke Gally\nBristol\nunknown\nKingston\n283.0\nNeale, Alexander\n\n\n2\n1714.0\n16111\nJacob\nBristol\nunknown\nKingston\n130.0\nNicholls, Philip\n\n\n3\n1714.0\n16112\nJason Gally\nBristol\nunknown\nPort Royal\n278.0\nPlummer, John\n\n\n5\n1714.0\n16114\nMercy Gally\nBristol\nAfrica, port unspecified\nBarbados, place unspecified\n190.0\nScott, John\n\n\n6\n1714.0\n16115\nMermaid Gally\nBristol\nCape Verde Islands\nKingston\n72.0\nBanbury, John&lt;br/&gt; Copinger, James\n\n\n8\n1714.0\n16117\nPeterborough\nBristol\nAfrica, port unspecified\nBarbados, place unspecified\n200.0\nShawe, John&lt;br/&gt; Martin, Joseph\n\n\n9\n1713.0\n16118\nResolution\nBristol\nGold Coast, port unspecified\nBarbados, place unspecified\n255.0\nWilliams, Charles\n\n\n10\n1714.0\n16119\nRichard and William\nBristol\nunknown\nPort Royal\n55.0\nBeckham, George&lt;br/&gt; Spring, Martin\n\n\n11\n1713.0\n16120\nRotchdale Gally\nBristol\nAfrica, port unspecified\nBarbados, place unspecified\n96.0\nHitchings, John\n\n\n12\n1714.0\n16121\nTunbridge Gally\nBristol\nAfrica, port unspecified\nBarbados, place unspecified\n200.0\nSkinner, Peter\n\n\n\n\n\n\n\n\nprint(\"Index) Column name\") \nfor i,name in enumerate(cleaned_df.columns): \n    print(i,\")\",name) \n\nIndex) Column name\n0 ) Year of arrival at port of disembarkation\n1 ) Voyage ID\n2 ) Vessel name\n3 ) Voyage itinerary imputed port where began (ptdepimp) place\n4 ) Voyage itinerary imputed principal place of slave purchase (mjbyptimp) \n5 ) Voyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\n6 ) Captives arrived at 1st port\n7 ) Captain's name"
  }
]