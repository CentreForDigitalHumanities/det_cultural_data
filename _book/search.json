[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data exploration toolkit for cultural data: structure, clean, visualize, and run a preliminary analysis",
    "section": "",
    "text": "Preface\nThis is a Quarto book"
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "1  Preface",
    "section": "",
    "text": "This is a Quarto book"
  },
  {
    "objectID": "parts/preliminaries/set_up.html",
    "href": "parts/preliminaries/set_up.html",
    "title": "2  Setting up",
    "section": "",
    "text": "Amazing set up instructions"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "7  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "parts/data_analysis/ch1_reading_data.html",
    "href": "parts/data_analysis/ch1_reading_data.html",
    "title": "4  Reading data",
    "section": "",
    "text": "from myutils.functions import question_box, list_to_html\n\nThe first thing we need to do is loading the data. This means taking the file where the data is currently stored and transfer that data here, in our working environment. As we are working with Python, this means transfering all the data in a Python object. Which object? There are Python libraries (code written by other developers) that are been specifically designed to the task of data analysis. One of these libraries, or (using the Python vocabulary) packages, is called pandas. So what we are going to do is reading our .csv file (coma separated file, where columns are separated by a coma) and storing the read information into a pandas DataFrame. How do we do all this? Using pandas and python specifically designed methods and functions.\n\ntask = 'Task: Loading data'\nprocess = list_to_html([\n    'Localise the csv file;',\n    'Have a look at it;',\n    'Transfering data to the Python working environment.'\n    ])\ntools = list_to_html([\n    'Python &lt;code&gt;pandas&lt;/code&gt; library;',\n    '&lt;pandas&gt; DataFrame to contain the data;',\n    '&lt;code&gt;.read_csv()&lt;/code&gt; method.',\n    ])\ncode = list_to_html([\n    \"&lt;code&gt;import pandas as pd&lt;/code&gt;, we first import the package &lt;code&gt;pandas&lt;/code&gt; in our working environment in order to use all its functionalities. \\\n    In order to tell Python we want to use &lt;code&gt;pandas&lt;/code&gt; functionalities we need to specify &lt;code&gt;pandas&lt;/code&gt; every time we use one of \\\n    its functions. To make our life easier, we assign to the package an alias, a nickname, so that we do not neet to write &lt;code&gt;pandas&lt;/code&gt; \\\n    all the times, but just the abbreviation &lt;code&gt;pd&lt;/code&gt;;\",\n    \"&lt;code&gt;data_file = 'data/data.csv'&lt;/code&gt;, we store the relative path of our data file as a string (between single quotes) to a Python variable called &lt;code&gt;data_file&lt;/code&gt;;\",\n    \"&lt;code&gt;df = pd.read_csv(data_file)&lt;/code&gt;, we use the &lt;code&gt;pandas&lt;/code&gt; method &lt;code&gt;.read_csv()&lt;/code&gt; to read our data file and we store the result on a Python variable called &lt;code&gt;df&lt;/code&gt; (data frame);\",\n    \"&lt;code&gt;print(type(df))&lt;/code&gt;, we first apply the Python function &lt;code&gt;type()&lt;/code&gt; to the just initialised variable df to check what is its type. We print the result on the screen using the Python function &lt;code&gt;print()&lt;/code&gt;.\"\n    ])\nquestion_box(task=task,tools=tools,process=process,code=code)\n\n \n            \n                    \n                        \n                        \n                            Task: Loading data\n                        \n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Localise the csv file;Have a look at it;Transfering data to the Python working environment.\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    Python pandas library; DataFrame to contain the data;.read_csv() method.\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    import pandas as pd, we first import the package pandas in our working environment in order to use all its functionalities.     In order to tell Python we want to use pandas functionalities we need to specify pandas every time we use one of     its functions. To make our life easier, we assign to the package an alias, a nickname, so that we do not neet to write pandas     all the times, but just the abbreviation pd;data_file = 'data/data.csv', we store the relative path of our data file as a string (between single quotes) to a Python variable called data_file;df = pd.read_csv(data_file), we use the pandas method .read_csv() to read our data file and we store the result on a Python variable called df (data frame);print(type(df)), we first apply the Python function type() to the just initialised variable df to check what is its type. We print the result on the screen using the Python function print().\n                       \n                \n        \n\n\n\nimport pandas as pd\ndata_file = 'data/data.csv'\ndf = pd.read_csv(data_file)\nprint(type(df))\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\nNice! We managed to transfer our data into a Python object, specifically a pandas.core.frame.DataFrame, or simply (from now on) a DataFrame. However, a lot of things can go wrong when going from one data format to another, so it is a good idea to have a first look at the data.\n\ntask = 'Task: Have a first look at the data'\nprocess = 'Visualize the first 10 lines of data, just to check that everything looks \"ok\"'\ntools = '&lt;code&gt;.head()&lt;/code&gt; method'\ncode = '&lt;code&gt;pd.head(10)&lt;/code&gt;, calling the &lt;code&gt;head(10)&lt;/code&gt; method on the DataFrame df we will visualise the first 10 lines of the DataFrame (we wrote 10, but you can use whatever number you want). \\\nThis method, as a matter of fact, shows you only the \"head\", the beginning, of your data.'\nquestion_box(task=task,tools=tools,process=process,code=code)\n\n \n            \n                    \n                        \n                        \n                            Task: Have a first look at the data\n                        \n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Visualize the first 10 lines of data, just to check that everything looks \"ok\"\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    .head() method\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    pd.head(10), calling the head(10) method on the DataFrame df we will visualise the first 10 lines of the DataFrame (we wrote 10, but you can use whatever number you want). This method, as a matter of fact, shows you only the \"head\", the beginning, of your data.\n                       \n                \n        \n\n\n\ndf.head(10)\n\n\n\n\n\n\n\n\nYear of arrival at port of disembarkation\nVoyage ID\nVessel name\nVoyage itinerary imputed port where began (ptdepimp) place\nVoyage itinerary imputed principal place of slave purchase (mjbyptimp)\nVoyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\nVOYAGEID2\nCaptives arrived at 1st port\nCaptain's name\n\n\n\n\n0\n1714.0\n16109\nFreeke Gally\nBristol\nNaN\nKingston\nNaN\n283.0\nNeale, Alexander\n\n\n1\n1713.0\n16110\nGreyhound Gally\nBristol\nNaN\nJamaica, place unspecified\nNaN\nNaN\nSelkirk, Alexander&lt;br/&gt; Forrest, Henry\n\n\n2\n1714.0\n16111\nJacob\nBristol\nNaN\nKingston\nNaN\n130.0\nNicholls, Philip\n\n\n3\n1714.0\n16112\nJason Gally\nBristol\nNaN\nPort Royal\nNaN\n278.0\nPlummer, John\n\n\n4\n1713.0\n16113\nLawford Gally\nBristol\nAfrica, port unspecified\nNewcastle (Nevis)\nNaN\nNaN\nStretton, Joseph\n\n\n5\n1714.0\n16114\nMercy Gally\nBristol\nAfrica, port unspecified\nBarbados, place unspecified\nNaN\n190.0\nScott, John\n\n\n6\n1714.0\n16115\nMermaid Gally\nBristol\nCape Verde Islands\nKingston\nNaN\n72.0\nBanbury, John&lt;br/&gt; Copinger, James\n\n\n7\n1713.0\n16116\nMorning Star\nBristol\nAfrica, port unspecified\nCharleston\nNaN\nNaN\nPoole, Nicholas\n\n\n8\n1714.0\n16117\nPeterborough\nBristol\nAfrica, port unspecified\nBarbados, place unspecified\nNaN\n200.0\nShawe, John&lt;br/&gt; Martin, Joseph\n\n\n9\n1713.0\n16118\nResolution\nBristol\nGold Coast, port unspecified\nBarbados, place unspecified\nNaN\n255.0\nWilliams, Charles\n\n\n\n\n\n\n\nComparing what we see here with our .csv file it seems that everything went well. We have the data organised in rows and columns. Each column has a name and the table is populated by our data. Some are numbers, some are names and places, some are symbols, some are NaN. We consider our data successfully loaded and ready to be used. It is not time yet to run data analysis, after having loaded the data we need to correctly understand which information contains, then we need to “clean” it, and after that, finally, we can proceed with some data analysis. This is just the beginning, but the best is yet to come!"
  },
  {
    "objectID": "parts/data_analysis/ch2_exploring_data.html",
    "href": "parts/data_analysis/ch2_exploring_data.html",
    "title": "5  Exploring data",
    "section": "",
    "text": "from myutils.functions import question_box, list_to_html\n\n\nimport pandas as pd\ndata_file = 'data/data.csv'\ndf = pd.read_csv(data_file)\nprint(type(df))\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\n\ndf.head(5)\n\n\n\n\n\n\n\n\nYear of arrival at port of disembarkation\nVoyage ID\nVessel name\nVoyage itinerary imputed port where began (ptdepimp) place\nVoyage itinerary imputed principal place of slave purchase (mjbyptimp)\nVoyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\nVOYAGEID2\nCaptives arrived at 1st port\nCaptain's name\n\n\n\n\n0\n1714.0\n16109\nFreeke Gally\nBristol\nNaN\nKingston\nNaN\n283.0\nNeale, Alexander\n\n\n1\n1713.0\n16110\nGreyhound Gally\nBristol\nNaN\nJamaica, place unspecified\nNaN\nNaN\nSelkirk, Alexander&lt;br/&gt; Forrest, Henry\n\n\n2\n1714.0\n16111\nJacob\nBristol\nNaN\nKingston\nNaN\n130.0\nNicholls, Philip\n\n\n3\n1714.0\n16112\nJason Gally\nBristol\nNaN\nPort Royal\nNaN\n278.0\nPlummer, John\n\n\n4\n1713.0\n16113\nLawford Gally\nBristol\nAfrica, port unspecified\nNewcastle (Nevis)\nNaN\nNaN\nStretton, Joseph\n\n\n\n\n\n\n\nNow that we correctly loaded our data in our working environment, it is time to figure out what the data contains. It is always a good idea to look at the dataset documentation (or metadata) to understand where the data comes from, what is the source of all the different records, how data have been collected, and any other possible data related caveat. Diving into documentation is up to you, in this chapter what we want to do is understanding as much as we can from the data itself, looking at its columns, rows, and values.  Every dataset tells a story. You may think about it like a person with a long experience, but not really willing to talk (well, some datasets “talks” more easily than others). It is your role in this case to “interrogate” this person, let him/her to talk, to tell a story and to dive into the details of that story, getting as much information as you can. This also depends on how much do you need to know: are you fine with a simple “chat” or you need the most accurate detailes?  Let’s formulate some questions to begin with: - How big is the data? - What does each row represent? - What does each column represent?\n\n\n \n            \n                    \n                        \n                        \n                            Question: How big is the data?\n                        \n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Counting the number of rows and columns and checking its size on the disk.\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    Python attribute shape\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    df.shape, the attribute .shape containes the size of the dataframe expressed in rows and columns.     When printed on the screen it will display two numbers, the first one being the number of rows and the second being the number of columns.\n                       \n                \n        \n\n\n\ndf.shape\n\n(36151, 9)\n\n\nOur DataFrame contains data distributed in 36151 rows and 9 columns. It is a quite big dataset. Shall we care how big is our dataset? We should as this may affect our analysis. For example, if we implement a scientific analysis that requires 1 second per row to produce an output, such program would take about 10hrs to analyse the entire dataset, and that is something we should keep in mind. That is why, in general, it is a good idea to test large analysis programs on a small sub-set of data and then, once verified that everything runs smoothly, performing the analysis on the entire dataset.\nLet’s continue exploring our DataFrame. We have 9 columns, we saw them displayed in our notebook and, luckily enough, their names are pretty descriptive, therefore in this case it is quite intuitive to understand what kind of information they contain. It could be useful to store the column names inside a Python variable and to display their names with a corresponding index (this will be useful later).\n\n\n \n            \n                    \n                        \n                        \n                            Display column names with an index\n                        \n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Identify column names and attributing them an index (starting from zero) depending on their order in the DataFrame (the first column index will be 0, the second 1, and so on).\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    pandas attribute .columns;Python for loop;Python function print().\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    column_names = df.columns, the DataFrame attributes .columns containes the column names of our     DataFrame. We store these names into the variable column_names;print(column_names), we use the function print() to print on the screen the content of the     variable column_names;i=0 \nprint(\"Index ) Column name\") \nfor name in column_names: \n    print(i,\")\",name) \n    i = i + 1 , we first initialise (assign a value) to the Python variable i, this will correspond to the first index.     We then print the string \"Index) Column name\" as a description for what we are going to print later. We finally use a     for loop to scroll the values contained in column_names. What the for loop does     is reading one by one the values stored in column_names and assigning them, one at the time, to the     variable name. It then performs all the instructions \"inside\" the loop (indented text) and it starts     all over again with the next value in column_names until all the values are explored. In our case, inside     the loop we perform just two operations: 1) we both print the current value of the variables i and     name and 2) we increase the value of i by 1. Why do we increase i? Because we     want to display the different column names according to their position in the DataFrame. Our loop automatically updates     the value of the variable name, but it does not increase by one step the index i, so we have     to do it explicitly.\n                       \n                \n        \n            \n                \n                    \n                    \n                        Expert Coding\n                    \n                \n                \n                    print(\"Index) Column name\") \nfor i,name in enumerate(column_names): \n    print(f\"{i}) {name}\") , you can substitute the previous block of code with this, it performs the same tasks (printing indices and column names) with less lines of code\n                       \n                \n        \n\n\n\ncolumn_names = df.columns\nprint(column_names)\ni=0 \nprint(\"Index ) Column name\") \nfor name in column_names: \n    print(i,\")\",name) \n    i = i + 1 \n\nIndex(['Year of arrival at port of disembarkation', 'Voyage ID', 'Vessel name',\n       'Voyage itinerary imputed port where began (ptdepimp) place',\n       'Voyage itinerary imputed principal place of slave purchase (mjbyptimp) ',\n       'Voyage itinerary imputed principal port of slave disembarkation (mjslptimp) place',\n       'VOYAGEID2', 'Captives arrived at 1st port', 'Captain's name'],\n      dtype='object')\nIndex ) Column name\n0 ) Year of arrival at port of disembarkation\n1 ) Voyage ID\n2 ) Vessel name\n3 ) Voyage itinerary imputed port where began (ptdepimp) place\n4 ) Voyage itinerary imputed principal place of slave purchase (mjbyptimp) \n5 ) Voyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\n6 ) VOYAGEID2\n7 ) Captives arrived at 1st port\n8 ) Captain's name\n\n\nNow we have the column names nicely listed from top to bottom and with their corresponding index assigned to them. You might be tempted to start the indexing from 1, but as in Python the first element of a list (or any other series of elements) has index 0, we also started from zero. You can also obtain the same result with less lines of code, try it out!\n\nprint(\"Index) Column name\") \nfor i,name in enumerate(column_names): \n    print(f\"{i}) {name}\") \n\nIndex) Column name\n0) Year of arrival at port of disembarkation\n1) Voyage ID\n2) Vessel name\n3) Voyage itinerary imputed port where began (ptdepimp) place\n4) Voyage itinerary imputed principal place of slave purchase (mjbyptimp) \n5) Voyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\n6) VOYAGEID2\n7) Captives arrived at 1st port\n8) Captain's name\n\n\nIt is now time to figure out what are the rows about. Looking at the column names, we notice that the second one (index 1) is called “Voyage ID”. This indicates that this column contains a specific identifier for the ship voyage, implying that each row contains specific information about a single trip. To verify that, we need to check if all the values of the Voyage ID column are different, i.e. if they are unique.\n\n\n \n            \n                    \n                        \n                        \n                            Are the values of the Voyage ID column unique?\n                        \n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Select the column Voyage ID, go through all its 36151 values and check if there are repetitions.\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    pandas column selector method .iloc;pandas attribute .is_unique;\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    voyage_id = df.iloc[:,1], we apply the method .iloc[] to select the second column     of our DataFrame and store it in the variable voyage_id. Inside the square brackets of .iloc[]     we can specify rows an columns to select in the form [,]. In this case,     : means that we select ALL the rows, so df.iloc[:,1] selects all the rows of the column     with index 1 (second column)print(voyage_id.is_unique), we print on the screen the pandas attribute .is_unique.     This attribute is True if, indeed, all the values of voyage_id is unique, False otherwise.\n                       \n                \n        \n            \n                \n                    \n                    \n                        Expert Coding\n                    \n                \n                \n                    print(df.iloc[:,1].is_unique), methods, attributes, and functions can be applied one after another in single line.\n                       \n                \n        \n\n\n\nvoyage_id = df.iloc[:,1]\nprint(voyage_id.is_unique)\n\nTrue\n\n\nWe verified that all the values of the Voyage ID column are unique, this means that all the rows of our DataFrame refer to a single ship voyage. Looking at the other column, we also notice that information where the voyage began, the port where slaves have been purchased, and the port where slaves have been desembarked is provided. This means that for single voyage we can intend the trip from a starting point to a slave purchase port that concludes in the port where slaves have been desembarked. Looking in particular at the fifth column (index 4, “Voyage itinerary imputed principal place of slave purchase”), we notice it contains several NaN. NaN stands for “Not a Number”, it is a value that appears when something goes wrong in one of the processes ran by our program. If something went wrong, why did not our program stop or tell us something about an occuring problem? Because problems may happen more often than you think and if our program stops working everytime it encounters a situation it cannot handle, it would most probably never finish running! In this case, most probably the record does not exists and it has either been indicated by NaN in our original .csv file or read by the pandas method .read_csv() as NaN.  In any case, the presence of NaN or any other missing value can severely affect our data analysis, for this reason before starting analysins the data we need to find and get rid of those value. This process is usually called “data cleaning” and that is exactly what we are going to do in the next chapter, we are going to clean our data."
  }
]