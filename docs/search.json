[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data exploration toolkit for cultural data",
    "section": "",
    "text": "Welcome!\n\nWelcome to the Cultural Data Exploration Toolkit! Throughout this workshop, you’ll explore the nuances of data-driven analysis, from constructing your dataset and formulating research inquiries to learning data visualisation techniques. Along this journey, you’ll contemplate the creation of personalised data models tailored to your research queries, navigate potential biases within datasets, and, importantly, learn how to effectively interrogate, explore, and analyse gathered information to generate visualisations."
  },
  {
    "objectID": "parts/supplements/schedule.html",
    "href": "parts/supplements/schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Time\nActivity\n\n\n\n\n13:00\nWelcome - Icebreaker\n\n\n13:15\nGroup activity: structuring data\n\n\n13:45\nDiscussion\n\n\n14:00\nBreak\n\n\n14:10\nIntroduction to data toolkit\n\n\n14:50\nBreak\n\n\n15:00\nExplorative Data Analyis\n\n\n16:30\nRecap & Questions"
  },
  {
    "objectID": "parts/preliminaries/set_up.html#running-the-analysis-locally-on-your-own-computer",
    "href": "parts/preliminaries/set_up.html#running-the-analysis-locally-on-your-own-computer",
    "title": "How to prepare for the workshop",
    "section": "Running the analysis locally on your own computer",
    "text": "Running the analysis locally on your own computer\n\nInstalling Python\n\nInstall python and jupyter notebook. For installation and setup we point at the “Introduction to Python & Data” Installation & Setup page;\n\n\n\nDownloading data and analysis code\n\nClick on this link, this will open a window in your browser with a green page and a single empty bar. Copy the address “https://github.com/CentreForDigitalHumanities/det_cultural_data/tree/main/playground” (without double quotes), paste it inside the bar and press ENTER. As soon as your press ENTER, your computer will start to download a zip file;\nExtract the content of the zip file:\n\nIf you are working on Windows, you can right-click on the zip file and, from the context menu, click on “Extract All”. A dialog box will appear asking where you want to extract the files. You can either use the default folder (same location as the ZIP file) or click “Browse” to select a different folder. We recomend to create a new folder called “cultural_data_explorative_analysis” and extract your files there;\nIf your are working on macOS, you just need to double click on the zip file. Your computer will automatically create an unzipped directoray with the same name of the zip file. We recomend to rename this directory “cultural_data_explorative_analysis”;\n\n\n\n\nOpen Jupyter notebook\n\nOpen Anaconda Navigator from the Start Menu (Windows Users) or from the Applications folder or Launchpad (macOS Users);\nClick on “Launch” under Jupyter Notebook. This will open a web browser;\nNavigate to the “cultural_data_explorative_analysis” folder;\nClick on the notebook file (.ipynb) to open it."
  },
  {
    "objectID": "parts/preliminaries/use_of_generative_ai.html#study-case",
    "href": "parts/preliminaries/use_of_generative_ai.html#study-case",
    "title": "Tips on using generative AI",
    "section": "Study case",
    "text": "Study case\nLet’s consider the very first piece of Python code you will find in this course:\nimport pandas as pd\n\ndata_file = 'data/data.csv'\ndf = pd.read_csv(data_file)\n\nprint(type(df))\nLet’s also assume that you have no idea about how Python works, so you will use generative AI copying and pasting this piece of code and ask “explain this”. This is what you might get:\n\nExplanation of the Code:\nimport pandas as pd\n\ndata_file = 'data/data.csv'\ndf = pd.read_csv(data_file)\n\nprint(type(df))\nThis code performs the following tasks:\n\nImporting the pandas library:\n\nThe line import pandas as pd imports the pandas library, which is used for data manipulation and analysis in Python. The alias pd is a common convention that makes it easier to reference pandas functions.\n\nDefining a file path:\n\nThe variable data_file is assigned the string 'data/data.csv', which represents the path to a CSV (Comma-Separated Values) file named data.csv. This file is located in a subdirectory called data, relative to the current working directory.\n\nReading the CSV file:\n\nThe line df = pd.read_csv(data_file) uses the read_csv() function from the pandas library to read the CSV file specified by the data_file variable. The data from the CSV is loaded into a pandas DataFrame, which is a 2-dimensional labeled data structure similar to a table.\nThe variable df now holds this DataFrame, which contains the data from the CSV file.\n\nPrinting the type of the DataFrame:\n\nThe line print(type(df)) prints the type of the variable df to the console. This is useful for confirming that the data was successfully loaded into a DataFrame.\n\n\n\nThis is a comprehensive explanation of what that piece of code does, but there may still be some room for improvement, especially if we are absolute beginners and we are not familiar with some of the technical language used in the description. Furthermore, not only we want to know what the code does, but we also want to take the opportunity to learn about programming. Let’s try with another prompt:\nAct as a software engineer, data scientist, and teacher. I am a bachelor student with no-prior knowledge in programming. I want to understand what code does and general programming concepts. Explain the following piece of code with the following output: 1) explanation of what the code does in few words, 2) line by line explanation of what the code does, 3) a list of fundamental programming concepts present in the piece of code with futher example, 4) for each programming concept, three references (website, books, tutorials, etc) to deepen my knowledge. Finally, create a glossary of the technical programming terms in a format that I can copy and paste in my notes. [attach here code in between double quotes]\nThis is the response:\n\nExplanation of the Code:\nimport pandas as pd\n\ndata_file = 'data/data.csv'\ndf = pd.read_csv(data_file)\n\nprint(type(df))\nThis code imports the pandas library, reads a CSV file into a DataFrame, and prints the type of the DataFrame to confirm successful data loading.\n\nLine-by-Line Explanation of What the Code Does\nimport pandas as pd\n\nWhat it does: This line imports the pandas library and assigns it the alias pd.\nPurpose: The alias pd is used for convenience, making it easier to reference pandas functions throughout the code.\n\ndata_file = 'data/data.csv'\n\nWhat it does: This line creates a variable named data_file and assigns it the string value 'data/data.csv', which is the path to a CSV file.\nPurpose: The variable holds the location of the file, making it easy to access and modify later.\n\ndf = pd.read_csv(data_file)\n\nWhat it does: This line uses the read_csv() function from the pandas library to read the CSV file specified in the data_file variable and stores the resulting DataFrame in the variable df.\nPurpose: It loads the data from the CSV file into a DataFrame, which allows for easy data manipulation and analysis.\n\nprint(type(df))\n\nWhat it does: This line prints the type of the variable df to the console.\nPurpose: It confirms that the data was loaded correctly by displaying the type of object df holds, which should be a pandas DataFrame.\n\n\nList of Fundamental Programming Concepts Present in the Code\n\nImporting Libraries/Modules:\n\nDescription: Using external libraries to enhance the functionality of your code.\nExample: import pandas as pd brings in the pandas library for data analysis.\n\nVariables:\n\nDescription: Named storage locations for data in a program.\nExample: data_file = 'data/data.csv' creates a variable named data_file.\n\nData Types:\n\nDescription: Categories of data that tell the compiler or interpreter how the programmer intends to use the data.\nExample: The variable data_file is a string, while df becomes a DataFrame.\n\nFunctions:\n\nDescription: A reusable block of code that performs a specific task.\nExample: pd.read_csv(data_file) is a function call that reads the CSV file.\n\nPrint Statements:\n\nDescription: Used to display output to the console.\nExample: print(type(df)) displays the type of the DataFrame.\n\n\n\nReferences for Each Programming Concept\nImporting Libraries/Modules\n\nW3Schools - Python Modules\nPython Documentation - Modules\nBook: “Automate the Boring Stuff with Python” by Al Sweigart\n\nVariables\n\nW3Schools - Python Variables\nReal Python - Python Variables\nBook: “Python Crash Course” by Eric Matthes\n\nData Types\n\nW3Schools - Python Data Types\nGeeksforGeeks - Data Types in Python\nBook: “Head First Python” by Paul Barry\n\nFunctions\n\nW3Schools - Python Functions\nReal Python - Defining Functions in Python\nBook: “Fluent Python” by Luciano Ramalho\n\nPrint Statements\n\nW3Schools - Python Print Function\nReal Python - The Python Print Function\nBook: “Learning Python” by Mark Lutz\n\n\nGlossary of Technical Programming Terms\n**Glossary of Technical Programming Terms**\n\n- **Importing Libraries/Modules**: The process of including external libraries in a program to use their functionality.\n- **Variables**: Named storage locations in a program that hold data values.\n- **Data Types**: Categories of data that determine the kind of operations that can be performed on that data (e.g., strings, integers, DataFrames).\n- **Functions**: Reusable blocks of code that perform specific tasks; they can take inputs (parameters) and return outputs.\n- **Print Statements**: Commands used to display output on the console or standard output device.\n\nAs you may notice, the generative AI ouput in now much more comprehensive. In this way we turned few lines of code in a small learning experience. There is still room for improvement, for example the concept of library (or package) is not explained and the examples provided are not more than those same lines of code you asked to explain. You may ask generative AI to be even more explicit in the explanation of technical terms and to provide different examples."
  },
  {
    "objectID": "parts/preliminaries/use_of_generative_ai.html#handling-errors",
    "href": "parts/preliminaries/use_of_generative_ai.html#handling-errors",
    "title": "Tips on using generative AI",
    "section": "Handling errors",
    "text": "Handling errors\nErrors are an inherent part of programming. When you run a program, encountering an error often means that something went wrong, causing the program to stop functioning. The root cause of an error can range from simple syntax issues, such as typos, to more complex problems. Regardless of the cause, diagnosing the issue can be time-consuming. Fortunately, generative AI can help streamline this troubleshooting process.\nWhen you encounter an error, you will typically see an error message—text that explains why the error occurred. For those new to programming, these messages can seem cryptic. However, they contain valuable clues that can assist you in identifying the problem. If you lack programming experience, generative AI can help you understand the error and suggest corrective actions.\nHere are some general tips to consider when dealing with errors:\n\nIdentify the Location of the Error: Error messages often indicate the line of code where the problem occurred. Before that line, check if the program runs correctly by dividing your code into smaller chunks and running them separately.\nDon’t Ignore Error Messages: Pay attention to the type of error message you receive. Use generative AI to explore common causes for that specific error type.\n*Check Syntax with Generative AI: Copy and paste the portion of your code that is generating the error into generative AI and ask it to review the syntax for potential typos or mistakes.\nProvide Specific Context: When querying generative AI, be explicit about your programming environment. Mention the programming language you are using (even though AI can often recognize it), the libraries involved, the task your program is performing, and any relevant data types you are working with."
  },
  {
    "objectID": "parts/preliminaries/intro_to_python.html",
    "href": "parts/preliminaries/intro_to_python.html",
    "title": "2  Introduction to Python",
    "section": "",
    "text": "Getting familiar with Jupyter Notebook\nJupyter Notebook is an open-source web application that allows you to create and share documents containing live code, equations, visualizations, and narrative text. It supports various programming languages, including Python, R, Julia, and more. However, it is most commonly used with Python.\nJupyter Notebook provides an interactive computing environment where you can write and execute code in a series of cells. Each cell can contain code, markdown text, equations, or visualizations. You can run individual cells or the entire notebook to see the output of the code and the results of any computations.\nThe name “Jupyter” is a combination of three programming languages: Julia, Python, and R, which were the first three languages supported by the Jupyter project. It was originally developed as part of the IPython project (hence the name “Jupyter”), but has since evolved into a language-agnostic tool that supports multiple programming languages.\nIn the context of Python, Jupyter Notebook is a popular tool for data analysis, scientific computing, machine learning, education, and research. It allows users to write, test, and document Python code in an interactive and visually appealing manner, making it a valuable tool for both beginners and experienced programmers alike.\nJupyter notebook cells can be either code, markdown, or raw. For the simple purpose of programming and writing text, ignore the raw option. You can easily shift between code and markdown selecting the cell, pressing Esc, and then M for markdown or Y for code.\nMarkdown is a language for formatting text, it allows you to quickly and easily create formatted documents using simple and intuitive syntax. This current cell and any other cell displaying text in this notebook, is written in markdown. You can learn the basics of markdown syntax in few minutes reading here or simply looking at the content of the text cells in this notebook and see what happens when you select them and run them.\nYou can tell if your cell selected cell is a code cell because you will see square brackets on its left ([ ]:).\nIf you want to delete a cell, use Esc + DD (press Esc and then d twice)\nWARNING: If your code cell has empty squred brackets, it means it has not been run YET.\nThere are some programming concepts that are common to all programming languages and can be found in any program:"
  },
  {
    "objectID": "parts/preliminaries/intro_to_python.html#variables-and-data-types",
    "href": "parts/preliminaries/intro_to_python.html#variables-and-data-types",
    "title": "2  Introduction to Python",
    "section": "Variables and data types",
    "text": "Variables and data types\nIn programming a variable is a container for a value. This value can either be a number, a string (a word), or any other type of programming object (we will talk about other possible objects later). Let’s initialise (define for the first time) some variables:\n\nname = 'Stefano'\nfavourite_planet = 'Saturn'\nbirth_day = 6\n\nIn the previous cell we stored the word Stefano into the variable name, the word Saturn into the variable favourite_planet, and the value 6 into the variable birth_day. From now on, every time we need to use one of these values in our programming, we just need to digit its corresponding variable name.\nIn Jupyter notebooks, if you want to check the value contained in a variable (so its content), you can simply run a cell with the variable name inside:\n\nname\n\n'Stefano'\n\n\n\nbirth_day\n\n6\n\n\n\nname\nbirth_day\n\n6\n\n\nAs you can notice, when you write different variable names in the same cell, only the last one will be printed on the screen."
  },
  {
    "objectID": "parts/preliminaries/intro_to_python.html#sequences-of-objects",
    "href": "parts/preliminaries/intro_to_python.html#sequences-of-objects",
    "title": "2  Introduction to Python",
    "section": "Sequences of objects",
    "text": "Sequences of objects\nWe can store single numbers and words inside a variable, but how about we want to store a sequence of values or words, or a mix of the two, into a variable? Of course we can, we just need to use a python object called list:\n\nnames = ['Stefano','Pippo','Alfio','Tano']\nages = [20,34,94,'unknown']\n\n\nnames[0]\n\n'Stefano'\n\n\n\nages[3]\n\n'unknown'\n\n\nIn Python lists are defined listing our sequence of values separated by coma inside square brackets (variable_name = [… , … , …]). Values stored in a list can be accessed using indexing. In python you count items starting from 0, so that the first item in a list has index 0. This means that for accessing the first item in the list names we will digit name[0], and to access the last item in the list ages we will digit ages[3]\nYou can create lists of any object, even lists of lists:\n\ninfo = [names,ages]\ninfo[0]\n\n['Stefano', 'Pippo', 'Alfio', 'Tano']\n\n\n\ninfo[0][0]\n\n'Stefano'\n\n\nIf we want to change a particular value in a list, we first need to access it and then we need to use the operator = to specify the new list value. For example, if we want to change ‘Stefano’ into ‘Steve’, we would do:\n\ninfo[0][0] = 'Steve'\ninfo[0]\n\n['Steve', 'Pippo', 'Alfio', 'Tano']"
  },
  {
    "objectID": "parts/preliminaries/intro_to_python.html#data-types-dictionaries",
    "href": "parts/preliminaries/intro_to_python.html#data-types-dictionaries",
    "title": "2  Introduction to Python",
    "section": "Data types: Dictionaries",
    "text": "Data types: Dictionaries\nIn python there are several ways you can store information. We just talked about lists, simple ordered sequences of objects. Another kind of data structure is called dictionary. In general a dictionary is a reference or resource that provides information, definitions, or explanations of words, terms, concepts, or objects. A dictionary is usually organised by alphabetically ordered words and by explanations associated to each word. In python a dictionary follows exactly the same organization principle: keyword and value.\n\ninfo_dict = {'name':'Stefano','favourite_number':6}\n\nTo define a dictionary we use curled brackets ({}) instead of squared brackets. Inside the curled brackets we need to specify couples of key/values separated by comas. To each key we can associate a different python object. Keys need to be unique, while values can by any Python object.\n\ninfo_dict = {'names':names}\n\n\ninfo_dict['names']\n\n['Steve', 'Pippo', 'Alfio', 'Tano']\n\n\nTo access the values contained inside a dictionary you cannot use numerical indices, as you would do for lists. Instead, you must use the name of the key related to the value. In the previous case the object names (a Python list) is associated with the key ‘names’. So, in order to access it, we need to digit info_dict['names'].\nIn a similar way, if you want to change the value related to a key, or create a new key/value couple, you first need to access that value and then to use the ‘=’ sign to assign a new value\n\ninfo_dict['names'] = ['Steve','Josef','Alfonse','Gerrit']\ninfo_dict['names']\n\n['Steve', 'Josef', 'Alfonse', 'Gerrit']"
  },
  {
    "objectID": "parts/preliminaries/intro_to_python.html#functions",
    "href": "parts/preliminaries/intro_to_python.html#functions",
    "title": "2  Introduction to Python",
    "section": "Functions",
    "text": "Functions\nA function is a python object that performs a single action given some parameters. In python, function names are usually verbs. If variable can be thought as subjects and objects in a sentence, functions are the verbs. Python has already some default functions, functions that are ready to use. Here you can find the build-in Python functions.\n\nresult = print(name)\nresult\n\nStefano\n\n\nThe way a function works is common to all programming languages: you give to the function one or more parameters, the function performs an action, and it returns a result. This happens so fast that, as a matter of fact, you can already think at a function and its parameters as its result. In the previous cell the function print() got as an input parameter the variable name and it printed its value on the screen.\n\nnumbers = [1,2,3,4,5,6,7,8,9,10]\nresult = sum(numbers)\nprint(result)\n\n55\n\n\nIn the previous cell, we defined a list of values (the first 10 integers), then we used the function sum to (guess what??) sum all the numbers in the list, and we stored the result into the variable result. We finally printed the result using the function print().\nBecause we know that variable values are printed automatically in Jupyter notebook cells when they contain the variable name, we could write directly:\n\nsum(numbers)\n\n55\n\n\nIndeed sum(numbers) represents an operation that returns the value 55 and can be considered equivalent to the value 55 itself, so that when we write it in a cell, we obtain the result printed on the screen.\n\nlen(numbers)\n\n10\n\n\nThe function len() is one of the most used function on objects containing many items. Indeed it tell us how many items are contained in that object (i.e. the length of that object). The function type() returns the type of a variable:\n\ntype(numbers)\n\nlist\n\n\nHow many functions are there? thousands, probably millions. Some of them have very intuitive names (like print() and sum()), some others have more complicated names. However, every function that can be used in Python comes with its own documentation, explaining which parameters it accepts, which additional options you can specify, and which kind of result you get back when applying it. To find about a function just google “ Python documentation” or ask ChatGPT about that."
  },
  {
    "objectID": "parts/preliminaries/intro_to_python.html#methods",
    "href": "parts/preliminaries/intro_to_python.html#methods",
    "title": "2  Introduction to Python",
    "section": "Methods",
    "text": "Methods\nMethods are functions that are object-specific. What does it mean? There are certain operations that can be perfomed only on a certain type of object. For example, if we consider a function that transforms lower characters into capital letters, it would not make much sense to apply this function to a number.\nAll objects in python can have their own specific functions and these object-specific functions are called methods. To use a method on an object, you need to apply the syntax &lt;object_name&gt;.method(). Do you see the difference compared to a general function syntax? In a general function we have function(par1,par2,...), while in a method we already know that the function, in this case method, will be applied to its object. Therefore, inside the parenthesis we only have additional parameters.\nLike functions, methods can accept all kind of parameters, but, of course, their main parameter is the object itself. Let’s see some example:\n\nname = 'Stefano Rapisarda Arthurus Micaelus'\nnumbers = [1,2,3,4,5]\n\n\nname.split()\n\n['Stefano', 'Rapisarda', 'Arthurus', 'Micaelus']\n\n\nWe initialised two variables: a string made of several words and a list of numbers. The split() method (a string-specific function) divides the string into a list of strings according to a separator. If you dont’ specify any separator (like in our case), white spaces will be considered as separators. Let’s see another example:\n\nnumbers.pop(2)\n\n3\n\n\n\nnumbers\n\n[1, 2, 4, 5]\n\n\nWe initialised two variables: a string made of several words and a list of numbers. The split() method (a string-specific function) divides the string into a list of strings according to a separator. If you dont’ specify any separator (like in our case), white spaces will be considered as separators. The variable numbers is already a list and using the method pop(x) we can remove the item occupying the 3rd position (index 2). The method affects the list and returns the just removed value.\nHow can we found about methods if there are so many? Usually a google search can point you at the method or function you need. In general you can always consult python documentation. You will find about string a list methods here and here, respectively."
  },
  {
    "objectID": "parts/preliminaries/intro_to_python.html#loops",
    "href": "parts/preliminaries/intro_to_python.html#loops",
    "title": "2  Introduction to Python",
    "section": "Loops",
    "text": "Loops\nOne of the potential of using machines is making them repeating the same operation hundreds, millions, or billions of times.\nLet’s say I have a list of names and I want to print them on the screen one by one:\n\nnames = ['James','Martin','Sandra','Paul','Chani']\nprint(names[0])\nprint(names[1])\nprint(names[2])\nprint(names[3])\nprint(names[4])\n\nJames\nMartin\nSandra\nPaul\nChani\n\n\nThis did not take us much time, because the names are only 5, but imagine you have a list of 1000 names; in that case printing all the names could take hours. Looking at the previous cell we notice that we use repeatedly the function print() using as input the values contained in the list names. Every time we need to repeat an operation many times, we can use a loop, specifically a for loop:\n\nfor i in range(5):\n    print(i,names[i])\n\n0 James\n1 Martin\n2 Sandra\n3 Paul\n4 Chani\n\n\nIn the previous cell the same operation (print()) is executed 5 times, but at each step, so at each iteration, the variable i changes, going from 0 to 4, one step at the time.\nIn order to achieve this result we need to start declaring for i. i is the variable name acting as a place holder for a value that will change at every step of the iteration. We chose the letter i, but you can choose any other name. After for i, we need to specify which values i can assume at each iteration. in range(5) means that i will go from 0 to 4, so it will increase of 1 integer per iteration stopping just before 5. Instead of a range of numbers, we can specify any other object containing several objects in it. In that case, the variable i (or whatever you will decide to call it), at each iteration, will be initialized with each value contained in the specified object. Let’s see some example:\n\nfor a in range(12): print(a)\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\nfor value in [0,1,4,5,6,7]:\n    print(value)\n\n0\n1\n4\n5\n6\n7\n\n\n\nfor name in names:\n    print(name)\n\nJames\nMartin\nSandra\nPaul\nChani\n\n\nFor looping over dictionaries, the concept is the same, but the syntax is a bit different because of the key/value stricture of dictionaries:\n\ninfo_dict = {\n    'name':'Stefano',\n    'surname':'Rapisarda',\n    'favourite_number':6,\n    'favourite_planet':'Saturn'\n}\nfor key,value in info_dict.items():\n    print(key,':',value)\n\nname : Stefano\nsurname : Rapisarda\nfavourite_number : 6\nfavourite_planet : Saturn\n\n\nWARNING You noticed that after the for statement, there is an indent of 4 spaces. You can make that indent using the TAB key. That indent tells python that that specific line of code is inside the look and, therefore, needs to be repeated. When you write code without indents, before or after the loop, those lines will be executed normally, i.e. once, one after the other.\n\nprint('Beginning of the for loop, we will have 10 iterations')\nprint('='*72)\nfor i in range(10):\n    print('This is iteration number:',i)\n    print('The next iteration will be:',i+1)\n    print('End of iteration',i)\n    print('-'*62)\nprint('='*72)\nprint('End of the loop')\n\nBeginning of the for loop, we will have 10 iterations\n========================================================================\nThis is iteration number: 0\nThe next iteration will be: 1\nEnd of iteration 0\n--------------------------------------------------------------\nThis is iteration number: 1\nThe next iteration will be: 2\nEnd of iteration 1\n--------------------------------------------------------------\nThis is iteration number: 2\nThe next iteration will be: 3\nEnd of iteration 2\n--------------------------------------------------------------\nThis is iteration number: 3\nThe next iteration will be: 4\nEnd of iteration 3\n--------------------------------------------------------------\nThis is iteration number: 4\nThe next iteration will be: 5\nEnd of iteration 4\n--------------------------------------------------------------\nThis is iteration number: 5\nThe next iteration will be: 6\nEnd of iteration 5\n--------------------------------------------------------------\nThis is iteration number: 6\nThe next iteration will be: 7\nEnd of iteration 6\n--------------------------------------------------------------\nThis is iteration number: 7\nThe next iteration will be: 8\nEnd of iteration 7\n--------------------------------------------------------------\nThis is iteration number: 8\nThe next iteration will be: 9\nEnd of iteration 8\n--------------------------------------------------------------\nThis is iteration number: 9\nThe next iteration will be: 10\nEnd of iteration 9\n--------------------------------------------------------------\n========================================================================\nEnd of the loop"
  },
  {
    "objectID": "parts/preliminaries/intro_to_python.html#conditional-statements",
    "href": "parts/preliminaries/intro_to_python.html#conditional-statements",
    "title": "2  Introduction to Python",
    "section": "Conditional statements",
    "text": "Conditional statements\nWe have seen how to store data and information into variables and how to access this information by indexing, so referring to the position of values inside an object. How about selecting information using other criteria? What about if we want to visualize only peoples names if they are older than 30 or printing the names of towns that start with an ‘s’? To do that in programming we need to use conditional statements. Conditional statements are indeed conditions that need to be satisfied in order for something to happen. What is “something”? Whatever action we want: an operation, a printing function, etc.\n\nfor key,value in info_dict.items():\n    if 'favourite' in key:\n        print(key,':',value)\n    else:\n        print('Not interested!')\n\nNot interested!\nNot interested!\nfavourite_number : 6\nfavourite_planet : Saturn\n\n\nWe used the same for loop to explore dictionaries keys and values, but this time, inside it, we wrote a conditional statement. The syntax for a conditional statement is:\nif &lt;condition&gt;: \n    action\nelse:\n    other_action\n&lt;condition&gt; is the condition that needs to be satisfied. In this case we want the word ‘favourite’ to be contained inside the key. If this happens, the condition is True and the “action” is perfomed (in our case, key and value will be printed). If the condition is False, the “other_action” will be perfomed (in our case, the ‘Not interested’ message will be printed).\nYou can also make conditions comparing quantities:\n\nnumbers = [1,2,3,4,5,6,7]\nfor number in numbers:\n    if number &lt; 4:\n        print(number,'is smaller than 4')\n    elif number &gt; 4:\n        print(number,'is larger than 4')\n    elif number == 4:\n        print(number,'is exactly 4')\n\n1 is smaller than 4\n2 is smaller than 4\n3 is smaller than 4\n4 is exactly 4\n5 is larger than 4\n6 is larger than 4\n7 is larger than 4\n\n\nConditional statements may also be combined:\n\nfor number in numbers:\n    if (number &lt; 4) or (number &gt; 4):\n        print(number, 'is not 4')\n    else:\n        print(number, 'must be 4')\n    \n\n1 is not 4\n2 is not 4\n3 is not 4\n4 must be 4\n5 is not 4\n6 is not 4\n7 is not 4\n\n\nIn the previous case we used three conditions that are satisfied if a number is smaller, larger, and equal to 4.\nUsing loops in combination with conditional statements is particularly useful when it’s time to select data. For example, imagine we have data in a table with two columns, one containes years and the other column can be any kind of measurement. In this case, you can use conditional statements to select measurements and very specific time intervals."
  },
  {
    "objectID": "parts/preliminaries/intro_to_python.html#packages",
    "href": "parts/preliminaries/intro_to_python.html#packages",
    "title": "2  Introduction to Python",
    "section": "Packages",
    "text": "Packages\nThere are millions of functions and objects out there, how can we use them? Python installation does not come with ALL the functions ever written for Python. Functions and objects are usually organized in packages (also called libraries or moduli). Each package contains a set of tools specific for certain tasks. There are tools for statistics, machine learning, building website, text-mining, etc. How can we access all these tools? First of all, we need to download the package into our computer. Usually in the documentation page of the package, there are installation instruction. Once installed, the package needs to be imported.\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nIn the previous cell we imported two packages, pandas and pyplot. When we import something, it is convenient to choose an alias for it, so that, when needed, we don’t need to write its entire name. In our case, pd will be the alias for pandas.\nIn the second line we see a slightly sifferent statement. In this case, we import the package pyplot. The package is a sub-package of the massive library matplotlib. Therefore, we need to specify the macro-package containing pyplot. We could also import pyplot in the following way.\n\nimport matplotlib.pyplot as plt\n\nFrom now on, every time we will need to use a pandas function or object, we just need to specify the alias of the package before the function or object we want to use:\n\ndf = pd.DataFrame()\n\nIn the previous case, we initialised a variable called pd with a pandas DataFrame. Let’s see what happens if we forget to specify pd:\n\ndf = DataFrame()\n\nNameError: name 'DataFrame' is not defined\n\n\nWe obtain an error because Python does not recognize the function name."
  },
  {
    "objectID": "parts/data_analysis/ch1_reading_data.html",
    "href": "parts/data_analysis/ch1_reading_data.html",
    "title": "3  Reading data",
    "section": "",
    "text": "The first thing we need to do is loading the data. This means opening the file where the data is currently stored and transfer that data here, in our working environment. As we are working with Python in this Jupyter notebook environment, this means transfering all the data into a Python object. Which object? There are Python libraries (Python code written by other developers) that have been specifically designed to perform the task of data analysis. One of these libraries, or (“Pythonically” speaking) packages, is called pandas. We will use one of the many pandas functions to read our .csv (coma separated values file) file and we will store the information into a pandas DataFrame.\n\n\n \n            \n                    \n                        \n                        \n                            Task(s)\n                        \n                    \n                    \n                        Loading data\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Localise the csv file;Have a look at it;Transfer data to the Python working environment.\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    pandas package;pandas method .read_csv();Python function print();Python function type()\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        import pandas as pd, we first import the package pandas in our working environment in order to use all its functionalities.     To make our life easier, we assign to the package an alias, a nickname, so that we do not neet to write     pandas.function_to_use() every time we need to use a pandas function. We need just to use     the abbreviated form pd.function_to_use();data_file = 'data/data.csv', we store the relative path of our data file as a string (between single     or double quotes) to the Python variable data_file;df = pd.read_csv(data_file), we use the method .read_csv() to read     our data file and we store the result in the Python variable df (data frame);print(type(df)), we first apply the Python function type() to the just initialised variable     df to check what is its type. We print the result on the screen using the Python function     print().\n                           \n                \n                \n        \n\n\n\nimport pandas as pd\ndata_file = 'data/data.csv'\ndf = pd.read_csv(data_file)\nprint(type(df))\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\nWe managed to transfer our data into a Python object, specifically a pandas.core.frame.DataFrame, or simply (from now on) a DataFrame. However, a lot of things can go wrong when going from one format to another, so it is a good idea to have a first look at the data.\n\n\n \n            \n                    \n                        \n                        \n                            Task(s)\n                        \n                    \n                    \n                        Have a first look at the data\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Visualize the first 10 lines of data, just to check that everything looks \"ok\";\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    pandas method .head();\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        pd.head(10), calling the head(10) method on the DataFrame df we will visualise the first 10 lines of the DataFrame (we wrote 10, but you can use whatever number you want). This method, as a matter of fact, shows you only the \"head\", the beginning, of the DataFrame.\n                           \n                \n                \n        \n\n\n\ndf.head(10)\n\n\n\n\n\n\n\n\nYear of arrival at port of disembarkation\nVoyage ID\nVessel name\nVoyage itinerary imputed port where began (ptdepimp) place\nVoyage itinerary imputed principal place of slave purchase (mjbyptimp)\nVoyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\nVOYAGEID2\nCaptives arrived at 1st port\nCaptain's name\n\n\n\n\n0\n1714.0\n16109\nFreeke Gally\nBristol\nNaN\nKingston\nNaN\n283.0\nNeale, Alexander\n\n\n1\n1713.0\n16110\nGreyhound Gally\nBristol\nNaN\nJamaica, place unspecified\nNaN\nNaN\nSelkirk, Alexander&lt;br/&gt; Forrest, Henry\n\n\n2\n1714.0\n16111\nJacob\nBristol\nNaN\nKingston\nNaN\n130.0\nNicholls, Philip\n\n\n3\n1714.0\n16112\nJason Gally\nBristol\nNaN\nPort Royal\nNaN\n278.0\nPlummer, John\n\n\n4\n1713.0\n16113\nLawford Gally\nBristol\nAfrica, port unspecified\nNewcastle (Nevis)\nNaN\nNaN\nStretton, Joseph\n\n\n5\n1714.0\n16114\nMercy Gally\nBristol\nAfrica, port unspecified\nBarbados, place unspecified\nNaN\n190.0\nScott, John\n\n\n6\n1714.0\n16115\nMermaid Gally\nBristol\nCape Verde Islands\nKingston\nNaN\n72.0\nBanbury, John&lt;br/&gt; Copinger, James\n\n\n7\n1713.0\n16116\nMorning Star\nBristol\nAfrica, port unspecified\nCharleston\nNaN\nNaN\nPoole, Nicholas\n\n\n8\n1714.0\n16117\nPeterborough\nBristol\nAfrica, port unspecified\nBarbados, place unspecified\nNaN\n200.0\nShawe, John&lt;br/&gt; Martin, Joseph\n\n\n9\n1713.0\n16118\nResolution\nBristol\nGold Coast, port unspecified\nBarbados, place unspecified\nNaN\n255.0\nWilliams, Charles\n\n\n\n\n\n\n\nComparing what we see here with our .csv file it seems that everything went well. We have the data organised in rows and columns. Each column has a name and each row and index. Looking at our data, some values are numbers, some are names and places, some contain htmlo tags, some are NaN. It is not time yet to run data analysis, after having loaded the data we still need to correctly interpret the information it contains, then we need to “clean” it, and after that, finally, we can proceed with some data analysis. This is just the beginning, but the best is yet to come!"
  },
  {
    "objectID": "parts/data_analysis/ch2_exploring_data.html#previous-steps",
    "href": "parts/data_analysis/ch2_exploring_data.html#previous-steps",
    "title": "4  Exploring data",
    "section": "Previous steps",
    "text": "Previous steps\n\nimport pandas as pd\ndata_file = 'data/data.csv'\ndf = pd.read_csv(data_file)\n\n\ndf.head(5)\n\n\n\n\n\n\n\n\nYear of arrival at port of disembarkation\nVoyage ID\nVessel name\nVoyage itinerary imputed port where began (ptdepimp) place\nVoyage itinerary imputed principal place of slave purchase (mjbyptimp)\nVoyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\nVOYAGEID2\nCaptives arrived at 1st port\nCaptain's name\n\n\n\n\n0\n1714.0\n16109\nFreeke Gally\nBristol\nNaN\nKingston\nNaN\n283.0\nNeale, Alexander\n\n\n1\n1713.0\n16110\nGreyhound Gally\nBristol\nNaN\nJamaica, place unspecified\nNaN\nNaN\nSelkirk, Alexander&lt;br/&gt; Forrest, Henry\n\n\n2\n1714.0\n16111\nJacob\nBristol\nNaN\nKingston\nNaN\n130.0\nNicholls, Philip\n\n\n3\n1714.0\n16112\nJason Gally\nBristol\nNaN\nPort Royal\nNaN\n278.0\nPlummer, John\n\n\n4\n1713.0\n16113\nLawford Gally\nBristol\nAfrica, port unspecified\nNewcastle (Nevis)\nNaN\nNaN\nStretton, Joseph\n\n\n\n\n\n\n\nNow that we correctly loaded our data in our working environment, it is time to figure out what the data contains. It is always a good idea to look at the dataset documentation (or metadata) to understand where the data comes from, what is the source of all the different records, how data has been collected, and any other possible data related caveat. Diving into the data documentation is up to you, in this chapter what we want to do is understanding as much as we can from the data itself, looking at its columns, rows, and values.  Every dataset tells a story. You may think about it like a person with a long experience, but not really willing to talk (well, some datasets “talk” more easily than others). It is your role in this case to “interrogate” the data, let it to talk, to tell a story and to dive into the details of that story, getting as much information as you can. This also depends on how much you need to know: will you be satisfied by a small “chat” or you need to know all kind of details?  Let’s formulate some questions to begin with.\n\n\n \n            \n                    \n                        \n                        \n                            Question(s)\n                        \n                    \n                    \n                        Question: How big is the data?\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Counting the number of rows and columns and checking its size on the disk.\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    pandas attribute shape\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        df.shape, the attribute .shape contains the size of the DataFrame expressed in rows and     columns. When printed on the screen it will display two numbers, number of rows and number of columns.\n                           \n                \n                \n        \n\n\n\ndf.shape\n\n(36151, 9)\n\n\n\nsolution = 'Our DataFrame contains data distributed in 36151 rows and 9 columns. '\nquestion_box(solution=solution)\n\n\n            \n                \n                    \n                    \n                        Answer\n                    \n                \n                \n                    Our DataFrame contains data distributed in 36151 rows and 9 columns. \n                \n                \n        \n\n\nIt is a quite big dataset. Shall we care about how big is our dataset? We should as this may affect our analysis. For example, if we implement a scientific analysis that requires 1 second per row to produce an output, such program would take about 10hrs to analyse the entire dataset, and that is something we should keep in mind. That is why, in general, it is a good idea to test large analysis programs on a small sub-set of data and then, once verified that everything runs smoothly, to perform the analysis on the entire dataset.\nLet’s continue exploring our DataFrame. We have 9 columns, we saw them displayed in our notebook and, luckily enough, their names are pretty descriptive, therefore, in this case, it is quite intuitive to understand what kind of information they contain. It could be useful to store the column names inside a Python variable and to display their names with a corresponding index (this will be useful later).\n\n\n \n            \n                    \n                        \n                        \n                            Task(s)\n                        \n                    \n                    \n                        Display the DataFrame column names with an index\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Identify column names and assign them an index (starting from zero) depending on their order in the DataFrame (the first column index will be 0, the second 1, and so on).\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    pandas attribute .columns;Python for loop;Python function print().\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        column_names = df.columns, the DataFrame attribute .columns contains the column names of our     DataFrame. We store these names into the variable column_names;print(column_names), we use the function print() to print on the screen the content of the     variable column_names;i=0\nprint(\"Index ) Column name\")\nfor name in column_names:\n    print(i,\")\",name)\n    i = i + 1, we first initialise (assign a value) to the Python variable i, this will correspond to the first index.     We then print the string \"Index) Column name\" as a description for what we are going to print later. We finally use a     for loop to scroll the values contained in column_names. What the for loop does     is reading one by one the values stored in column_names and assigning them, one at the time, to the     variable name. It then performs all the instructions \"inside\" the loop (indented text) and, once all the     instructions are executed, it starts     all over again with the next value in column_names until all the values are explored. In our case, inside     the loop we perform just two operations: 1) we both print the current value of the variables i and     name and 2) we increase the value of i by 1. Why do we increase i? Because we     want to display the different column names according to their position in the DataFrame. Our loop automatically updates     the value of the variable name, but it does not increase by one step the index i, so we have     to do it explicitly.\n                           \n                \n                \n        \n            \n                \n                    \n                    \n                        Expert Coding\n                    \n                \n                \n                    \n                        print(\"Index) Column name\") \nfor i,name in enumerate(column_names): \n    print(f\"{i}) {name}\") , you can substitute the previous block of code with this, it performs the same tasks (printing indices and column names) with less lines of code\n                      \n                \n                \n        \n\n\n\ncolumn_names = df.columns\nprint(column_names)\ni=0 \nprint(\"Index ) Column name\") \nfor name in column_names: \n    print(i,\")\",name) \n    i = i + 1 \n\nIndex(['Year of arrival at port of disembarkation', 'Voyage ID', 'Vessel name',\n       'Voyage itinerary imputed port where began (ptdepimp) place',\n       'Voyage itinerary imputed principal place of slave purchase (mjbyptimp) ',\n       'Voyage itinerary imputed principal port of slave disembarkation (mjslptimp) place',\n       'VOYAGEID2', 'Captives arrived at 1st port', 'Captain's name'],\n      dtype='object')\nIndex ) Column name\n0 ) Year of arrival at port of disembarkation\n1 ) Voyage ID\n2 ) Vessel name\n3 ) Voyage itinerary imputed port where began (ptdepimp) place\n4 ) Voyage itinerary imputed principal place of slave purchase (mjbyptimp) \n5 ) Voyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\n6 ) VOYAGEID2\n7 ) Captives arrived at 1st port\n8 ) Captain's name\n\n\nNow we have the column names nicely listed from top to bottom and with their corresponding index assigned to them. You might be tempted to start the indexing from 1, but as in Python the first element of a list (or any other series of elements) has index 0, we started counting from zero. You can obtain the same result with less lines of code, try it out!\n\nprint(\"Index) Column name\") \nfor i,name in enumerate(column_names): \n    print(f\"{i}) {name}\") \n\nIndex) Column name\n0) Year of arrival at port of disembarkation\n1) Voyage ID\n2) Vessel name\n3) Voyage itinerary imputed port where began (ptdepimp) place\n4) Voyage itinerary imputed principal place of slave purchase (mjbyptimp) \n5) Voyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\n6) VOYAGEID2\n7) Captives arrived at 1st port\n8) Captain's name\n\n\nIt is now time to figure out what are the rows about. Looking at the column names, we notice that the second one (index 1) is called “Voyage ID”. This indicates that this column contains a specific identifier for the ship voyage, implying that each row contains specific information about a single trip. To verify that each row corresponds to a single voyage, we need to check if all the values of the Voyage ID column are different, i.e. if they are unique.\n\n\n \n            \n                    \n                        \n                        \n                            Question(s)\n                        \n                    \n                    \n                        Are the values of the Voyage ID column unique?\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Select the column Voyage ID, go through all its 36151 values and check if there are repetitions.\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    pandas column selector method .iloc[];pandas attribute .is_unique;Python function print()\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        voyage_id = df.iloc[:,1], we apply the method .iloc[] to select the second column     of our DataFrame and store it in the variable voyage_id. Inside the square brackets of .iloc[]     we can specify rows an columns to select in the form [selected_rows,selected_columns]. In this case,     : means that we select ALL the rows, so df.iloc[:,1] selects all the rows of the column     with index 1 (second column)print(voyage_id.is_unique), we print on the screen the pandas attribute .is_unique.     This attribute is True if, indeed, all the values of voyage_id are unique, False otherwise.\n                           \n                \n                \n        \n            \n                \n                    \n                    \n                        Expert Coding\n                    \n                \n                \n                    \n                        print(df.iloc[:,1].is_unique), methods, attributes, and functions can be applied one after another     in a single line of code.\n                      \n                \n                \n        \n\n\n\nvoyage_id = df.iloc[:,1]\nprint(voyage_id.is_unique)\n\nTrue\n\n\nWe verified that all the values of the Voyage ID column are unique, this means that each of the rows of our DataFrame refers to a single ship voyage. Looking at the other columns, we also notice that information where the voyage began, the port where slaves have been purchased, and the port where slaves have been desembarked is provided. Looking in particular at the fifth column (index 4, “Voyage itinerary imputed principal place of slave purchase”), we notice it contains several NaNs. NaN stands for “Not a Number”, it is a value that appears when something goes wrong in one of the processes ran by our program. If something went wrong, why did not our program stop or told us something about an occuring problem? Because problems may happen more often than you think and if our program stops working everytime it encounters a situation it cannot handle, it would most probably never finish running! In this case, most probably the record does not exist so the data set cell has been filled by NaN, either in our original .csv file or by the pandas method .read_csv(). NaN are not necesseraly something bad, as they can be easily identified and eventually corrected (or simply ignored). Incorrect or missing data may be much harder to spot and correct.  In any case, the presence of NaNs or any other missing value can severely affect our data analysis, for this reason before starting analysing the data we need to find and get rid of those values. This process is usually called “data cleaning” and that is exactly what we are going to do in the next chapter."
  },
  {
    "objectID": "parts/data_analysis/ch3_data_cleaning.html#previous-steps",
    "href": "parts/data_analysis/ch3_data_cleaning.html#previous-steps",
    "title": "5  Cleaning data",
    "section": "Previous steps",
    "text": "Previous steps\n\nimport pandas as pd\ndata_file = 'data/data.csv'\ndf = pd.read_csv(data_file)\nprint(df.shape)\n\n(36151, 9)\n\n\n\ncolumn_names = df.columns\ndf.head(5)\n\n\n\n\n\n\n\n\nYear of arrival at port of disembarkation\nVoyage ID\nVessel name\nVoyage itinerary imputed port where began (ptdepimp) place\nVoyage itinerary imputed principal place of slave purchase (mjbyptimp)\nVoyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\nVOYAGEID2\nCaptives arrived at 1st port\nCaptain's name\n\n\n\n\n0\n1714.0\n16109\nFreeke Gally\nBristol\nNaN\nKingston\nNaN\n283.0\nNeale, Alexander\n\n\n1\n1713.0\n16110\nGreyhound Gally\nBristol\nNaN\nJamaica, place unspecified\nNaN\nNaN\nSelkirk, Alexander&lt;br/&gt; Forrest, Henry\n\n\n2\n1714.0\n16111\nJacob\nBristol\nNaN\nKingston\nNaN\n130.0\nNicholls, Philip\n\n\n3\n1714.0\n16112\nJason Gally\nBristol\nNaN\nPort Royal\nNaN\n278.0\nPlummer, John\n\n\n4\n1713.0\n16113\nLawford Gally\nBristol\nAfrica, port unspecified\nNewcastle (Nevis)\nNaN\nNaN\nStretton, Joseph\n\n\n\n\n\n\n\nNow that we got some familiarity with our dataset, it is time to clean our data, i.e. to get rid of all those NaN values and anything else that might effect our data analysis. Where to start? Well, inspecting the DataFrame by eye, we see several NaNs in the first 5 rows of our DataFrame. The first column we see NaNs is “Voyage itinerary imputed principal place of slave purchase”, the fourth column (index 5). It would be nice to check if also other column have NaNs. Let’s start with the first column, “Year of arrival at port of disembarkation” (index 0), let’s check if this column contains any NaN and then we will repeat the same process for all the other columns.\n\n\n \n            \n                    \n                        \n                        \n                            Question(s)\n                        \n                    \n                    \n                        Is there any NaN in the first column? How many are they?\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Go through all the 36151 entries of the first column and look for NaN values.\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    pandas column selector method .iloc;pandas method .is_na;Python function .sum();Python function print().\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        arr_year = df.iloc[:,0], we first select the first column of the DataFrame using the     pandas method .iloc[] and we store the selected column in the new     variable arr_year (we already used this method in the previous chapter);arr_year_na = arr_year.isna(), we apply the pandas method .isna().     When we apply the .isna() method, we obtain a result with the same dimensions of the     object we applied it to, containing either True or False depending on if the corresponding value is     a NaN (or na, non arithmetic) value or not. Indeed the result of this method just answers the question:     is this value na?     We store the result in the variable arr_year_na (you are free to use a more descriptive name);print(arr_year_na.sum()), in Python the boolean values True and False are equivalent to 1 and 0,     respectively. This means that     if we have an array (a list or sequence of values) containing True and False, if we sum all the values,     we would obtain the number of True values (as they count as 1 and everything else counts as 0). These     True values correspond to the cases when  the method .isna() found a NaN, so that summing     all these values means, as a matter of fact, counting how many NaNs have been found.\n                           \n                \n                \n        \n            \n                \n                    \n                    \n                        Expert Coding\n                    \n                \n                \n                    \n                        print(df.iloc[:,0].isna()), we can apply methods and function one after another, using less coding lines and saving space in our computer memory\n                      \n                \n                \n        \n\n\n\narr_year = df.iloc[:,0]\narr_year_na = arr_year.isna()\nprint(arr_year_na)\nprint('Total number of NaNs in the first column:',arr_year_na.sum())\n\n0        False\n1        False\n2        False\n3        False\n4        False\n         ...  \n36146    False\n36147    False\n36148    False\n36149    False\n36150    False\nName: Year of arrival at port of disembarkation, Length: 36151, dtype: bool\nTotal number of NaNs in the first column: 1\n\n\n\nsolution = 'The first column contains 1 NaN value'\nquestion_box(solution=solution)\n\n\n            \n                \n                    \n                    \n                        Answer\n                    \n                \n                \n                    The first column contains 1 NaN value\n                \n                \n        \n\n\nIn this way we found our that the first column has 1 NaN (or na) value, that would have been quite hard to spot by eye scrolling 36151 lines!  It is great that we found 1 NaN in the first column, but where exactly it is located? What’s the corresponding Voyage ID of that value?\n\n\n \n            \n                    \n                        \n                        \n                            Question(s)\n                        \n                    \n                    \n                        Where is the NaN value located in the first column?\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Go through all the 36151 entries of the first column and look for the NaN value.\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    Python masking.\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        df[arr_year_na], here we will use one of the most useful features when working with     DataFrames: masking. From our previous coding, arr_year_na is an object with the same shape and     features of the first column, but instead of containing years, it contains True and False values,     where True corresponds to NaNs found applying the method .isna(). If we consider     our DataFrame df, we can use this object to select data from it. What are we going to     select? Well, arr_year_na has a name, and that is the name of the first column, so we     will select that. We will also select only the rows where arr_year_na is True.\n                           \n                \n                \n        \n\n\n\ndf[arr_year_na]\n\n\n\n\n\n\n\n\nYear of arrival at port of disembarkation\nVoyage ID\nVessel name\nVoyage itinerary imputed port where began (ptdepimp) place\nVoyage itinerary imputed principal place of slave purchase (mjbyptimp)\nVoyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\nVOYAGEID2\nCaptives arrived at 1st port\nCaptain's name\n\n\n\n\n32248\nNaN\n91909\nKitty\nLiverpool\nNaN\nNaN\nNaN\nNaN\nFisher, Joseph\n\n\n\n\n\n\n\n\n\n\n            \n                \n                    \n                    \n                        Answer\n                    \n                \n                \n                    The row containing a NaN in the first column (\"Year of arrival at port of disembarkation\") has index 32248, it misses information about its itinerary (only the starting port is present) and number of captives, and its ID is 91909\n                \n                \n        \n\n\nIn this way we can inspect NaNs one by one and we can make a decision about how to handle them. In our DataFrame there are thousands of NaNs (as you will see in a minute) and going through ALL of them one by one is not a good idea. Let’s first try to figure out if the other columns have also NaNs and how many are they. The process will be quite straightforward as we already did it for one of the columns, so what we need to do now is to repeat the same procedure for all the other columns.\n\n\n \n            \n                    \n                        \n                        \n                            Question(s)\n                        \n                    \n                    \n                        Which column has NaNs? How many are they?\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Go through the DataFrame columns one by one and count the NaNs\n                \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        for column_name in column_names:\n    selected_column = df[column_name]\n    selected_column_na = selected_column.isna()\n    n_nan = selected_column_na.sum()\n    print(column_name,\"has\",n_nan,\"NaN\"), we start with a for loop scrolling all the elements contained in the variable column_names.     In this variable we previously stored all the column names. In this way the variable column_name     will containg a single column name for each iteration of the loop. We then use the column name to select that     specific column in our DataFrame, storing this selected column in the variable selected_column.     We apply the method .isna() to the selected column to obtain, instead of a column of numerical or     string values, a column of True and False (boolean) values. We use the method .sum() to sum all     the True and False values. Keeping in mind that True is equivalent to 1 and False to 0, this will result in     counting all the NaN values in the selected column. We store this last result in the variable n_nan.     We finally print this result, together with the name of the selected column, using the python function print().\n                           \n                \n                \n        \n            \n                \n                    \n                    \n                        Expert Coding\n                    \n                \n                \n                    \n                        for i,column_name in enumerate(column_names):\n    print(f\"{i}) {column_name} has {df[column_name].isna().sum()} NaN\"), we can apply the methods described above one after another, without storing any intermidiate result in a new     variable. We can also use the python function enumerate() to not only scroll through the df     column names, but also for counting the loop iterations starting from the first one (index 0). The iteration index     will be stored in the variable i.\n                      \n                \n                \n        \n\n\n\nfor column_name in column_names:\n    selected_column = df[column_name]\n    selected_column_na = selected_column.isna()\n    n_nan = selected_column_na.sum()\n    print(column_name,'has',n_nan,'NaN')\n\nYear of arrival at port of disembarkation has 1 NaN\nVoyage ID has 0 NaN\nVessel name has 1614 NaN\nVoyage itinerary imputed port where began (ptdepimp) place has 4508 NaN\nVoyage itinerary imputed principal place of slave purchase (mjbyptimp)  has 2210 NaN\nVoyage itinerary imputed principal port of slave disembarkation (mjslptimp) place has 4191 NaN\nVOYAGEID2 has 36101 NaN\nCaptives arrived at 1st port has 17743 NaN\nCaptain's name has 4028 NaN\n\n\nand if we want to keep in mind the column index of each column…\n\nfor i,column_name in enumerate(column_names): \\\n    print(f\"{i}) {column_name} has {df[column_name].isna().sum()} NaN\")\n\n0) Year of arrival at port of disembarkation has 1 NaN\n1) Voyage ID has 0 NaN\n2) Vessel name has 1614 NaN\n3) Voyage itinerary imputed port where began (ptdepimp) place has 4508 NaN\n4) Voyage itinerary imputed principal place of slave purchase (mjbyptimp)  has 2210 NaN\n5) Voyage itinerary imputed principal port of slave disembarkation (mjslptimp) place has 4191 NaN\n6) VOYAGEID2 has 36101 NaN\n7) Captives arrived at 1st port has 17743 NaN\n8) Captain's name has 4028 NaN\n\n\n\n\n\n            \n                \n                    \n                    \n                        Answer\n                    \n                \n                \n                    The only column that does not have NaNs is the Voyage ID column, all the others (excluding the Year of Arrival [...]) have thousands of NaNs.\n                \n                \n        \n\n\nAt this point we have a general idea of the amount of data missing in our DataFrame. The following question is how to deal with this missing data? There are several things we can do, the easiest option would be just exclude it from our DataFrame. However, in order to answer a research question, we often do not need to use or explore ALL the available information and we would usually be interested in some parameters more than others. In this case our data selection could be performed looking at one or more specific columns. What to do with the rest of the NaNs? We can either leave them as they are and trying to figure out how our analysis program will “digest” these values or find good substitute for them. The value of this substitute will depend on the data type of the columns containing the NaN and on our decision. For example the NaN in the columns containing a descriptive string, like the vessel name or the starting port, could be substituted by the string “unknown”. NaNs in the “Captives arrived […]” column could be left as they are (you may be tempted to change them to 0, but zero captives is quite different from unknown number of captives) or substituted by, for example, the average of captives during the same year.  Each choice will have different implications to our final results, the most important thing in this stage is to clearly document our criteria for filtering NaN. In our specific case we will be mostly interested in the data containing the number of captives, so we want to filter our all those rows where the number of captives is NaN. We will then exclude the columns VOYAGEID2 as we already have a voyage ID and it is not listed in the data variable description. To resume, here there are our cleaning criteria: - All the rows not containing data about the number of captives have been removed; - All the NaN values in columns with descriptive information (e.g. names) have been substituted with “unknown”; - The column VOYAGEID2 has been removed from the DataFrame.\n\n\n \n            \n                    \n                        \n                        \n                            Task(s)\n                        \n                    \n                    \n                        Cleaning data\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Remove all the data of the column VOYAGEID2;Go through all the values of the column \"Captives arrived at 1st port\" and remove rows containing NaNs;Go through all the values of the descriptive columns and change NaN into \"unknown\"Check how much data has been filtered out.\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    Python function print();pandas attribute .columns;pandas method drop();pandas method .head();pandas method .dropna();pandas method .fillna();pandas attribute .shape;Python function len();Python operators * (multiplication), / (division), and - (subtraction)\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        print(df.columns), we start printing the name of our DataFrame columns, just as a reminder. We use     the Python function print() and the pandas attribute .columns that     we already used several times;column_to_remove = \"VOYAGEID2\"\ncolumn_to_remove_nan = \"Captives arrived at 1st port\", we store the name of the columns we are interested in into two variables. This is not stricktly     necessary, as we could use the name of the columns directly as input for methods and functions. However, with this     extra step, our code will be more readable as we are using two very descriptive variable names: column_to_remove     will, indeed, contain the name of the column we want to remove from our DataFrame (VOYAGEID2) and column_to_remove_nan     will contain the name of the column to use as a reference for filtering rows, all the rows having NaN in this column     will be removed;cleaned_df_step1 = df.drop(column_to_remove,axis=1), we use the method .drop() to remove     a specific column. Each row of a pandas DataFrame has an index. This index is different from the index     that every Python list of objects has, i.e. an integer number starting from 0. This DataFrame index is an additional     index that can be not only a number, but also a string or a label. The pandas method .drop()     can be used either to remove columns, depending on column names, or rows, depending on pandas index.     To indicate that we want to target columns, and not rows, we need to specify the parameter axis equal     to 1 (0 would target the rows). Together with that, we also need to specify the name of the column we want to remove, stored     in the variable column_to_remove. In this way, we apply the method .drop() to our \"dirty\"     DataFrame df and we store the result into the variable cleaned_df_step1;cleaned_df_step1.head(5), after every filtering operation, it is a good idea to check out the effect on     our output result. In this case we use the pandas method .head() to visualise the first five     rows of our DataFrame cleaned_df_step1cleaned_df_step2 = cleaned_df_step1.dropna(subset=[column_to_remove_nan]), we use the method .dropna()     to filter out all the rows having NaN in a list of columns. The list of columns needs to be specified in the function     parameter subset. In this case we have a single column name stored in the variable column_to_remove_nana     and to make it a list we put this variable in between square brackets [ ]. We apply the method to the     DataFrame cleaned_df_step1 and store the result in cleaned_df_step2;cleaned_df = cleaned_df_step2.fillna(\"unknown\"), we finally use the method .fillna() to fill all     the NaN in our DataFrame with a certain value, in this case the string (word) \"unknown\". We apply the method to the     DataFrame cleaned_df_step2 and store the result into cleaned_df;print(cleaned_df.shape)\nn_filtered_rows = len(df)-len(cleaned_df)\nper_cent = (n_filtered_rows/len(df))*100, this block of Python instructions checks the size of our filtered DataFrame cleaned_df and     compares it with our original, raw, DataFrame df. We first visualise the size of cleaned_df     using the attribute .shape, then we obtain simply the number of rows of each DataFrame using the Python     function len(). We subtract the number of rows of the DataFrames before and after filtering to check how many     rows we filtered out and we store the result into the variable n_filtered_rows. We finally compute the percent     of filtered rows compared to the initial size of the DataFrame.\n                           \n                \n                \n        \n\n\n\n# Display the name of the columns first\nprint(df.columns)\n\n# Select our target columns for clearning the data\ncolumn_to_remove = 'VOYAGEID2'\ncolumn_to_remove_nan = 'Captives arrived at 1st port'\n\n# Perform Data Cleaning visualising the result step by step\n# step1, removing column VOYAGEID2 from the DataFrame\ncleaned_df_step1 = df.drop(column_to_remove,axis=1)\ncleaned_df_step1.head(5)\n\nIndex(['Year of arrival at port of disembarkation', 'Voyage ID', 'Vessel name',\n       'Voyage itinerary imputed port where began (ptdepimp) place',\n       'Voyage itinerary imputed principal place of slave purchase (mjbyptimp) ',\n       'Voyage itinerary imputed principal port of slave disembarkation (mjslptimp) place',\n       'VOYAGEID2', 'Captives arrived at 1st port', 'Captain's name'],\n      dtype='object')\n\n\n\n\n\n\n\n\n\nYear of arrival at port of disembarkation\nVoyage ID\nVessel name\nVoyage itinerary imputed port where began (ptdepimp) place\nVoyage itinerary imputed principal place of slave purchase (mjbyptimp)\nVoyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\nCaptives arrived at 1st port\nCaptain's name\n\n\n\n\n0\n1714.0\n16109\nFreeke Gally\nBristol\nNaN\nKingston\n283.0\nNeale, Alexander\n\n\n1\n1713.0\n16110\nGreyhound Gally\nBristol\nNaN\nJamaica, place unspecified\nNaN\nSelkirk, Alexander&lt;br/&gt; Forrest, Henry\n\n\n2\n1714.0\n16111\nJacob\nBristol\nNaN\nKingston\n130.0\nNicholls, Philip\n\n\n3\n1714.0\n16112\nJason Gally\nBristol\nNaN\nPort Royal\n278.0\nPlummer, John\n\n\n4\n1713.0\n16113\nLawford Gally\nBristol\nAfrica, port unspecified\nNewcastle (Nevis)\nNaN\nStretton, Joseph\n\n\n\n\n\n\n\n\n# step2, removing all the rows haveing NaN in the \"Captives arrived at 1st port\" column\ncleaned_df_step2 = cleaned_df_step1.dropna(subset=[column_to_remove_nan])\ncleaned_df_step2.head(5)\n\n\n\n\n\n\n\n\nYear of arrival at port of disembarkation\nVoyage ID\nVessel name\nVoyage itinerary imputed port where began (ptdepimp) place\nVoyage itinerary imputed principal place of slave purchase (mjbyptimp)\nVoyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\nCaptives arrived at 1st port\nCaptain's name\n\n\n\n\n0\n1714.0\n16109\nFreeke Gally\nBristol\nNaN\nKingston\n283.0\nNeale, Alexander\n\n\n2\n1714.0\n16111\nJacob\nBristol\nNaN\nKingston\n130.0\nNicholls, Philip\n\n\n3\n1714.0\n16112\nJason Gally\nBristol\nNaN\nPort Royal\n278.0\nPlummer, John\n\n\n5\n1714.0\n16114\nMercy Gally\nBristol\nAfrica, port unspecified\nBarbados, place unspecified\n190.0\nScott, John\n\n\n6\n1714.0\n16115\nMermaid Gally\nBristol\nCape Verde Islands\nKingston\n72.0\nBanbury, John&lt;br/&gt; Copinger, James\n\n\n\n\n\n\n\n\n# step3, changing all the other NaN into unknown\ncleaned_df = cleaned_df_step2.fillna(\"unknown\")\ncleaned_df.head(5)\n\n\n\n\n\n\n\n\nYear of arrival at port of disembarkation\nVoyage ID\nVessel name\nVoyage itinerary imputed port where began (ptdepimp) place\nVoyage itinerary imputed principal place of slave purchase (mjbyptimp)\nVoyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\nCaptives arrived at 1st port\nCaptain's name\n\n\n\n\n0\n1714.0\n16109\nFreeke Gally\nBristol\nunknown\nKingston\n283.0\nNeale, Alexander\n\n\n2\n1714.0\n16111\nJacob\nBristol\nunknown\nKingston\n130.0\nNicholls, Philip\n\n\n3\n1714.0\n16112\nJason Gally\nBristol\nunknown\nPort Royal\n278.0\nPlummer, John\n\n\n5\n1714.0\n16114\nMercy Gally\nBristol\nAfrica, port unspecified\nBarbados, place unspecified\n190.0\nScott, John\n\n\n6\n1714.0\n16115\nMermaid Gally\nBristol\nCape Verde Islands\nKingston\n72.0\nBanbury, John&lt;br/&gt; Copinger, James\n\n\n\n\n\n\n\n\n# step4, checking how much data we filtered out\nprint(cleaned_df.shape)\nn_filtered_rows = len(df)-len(cleaned_df)\nper_cent = (n_filtered_rows/len(df))*100\nprint('We filtered out: ',len(df)-len(cleaned_df),', corresponding to about', round(per_cent), '% of our initial data')\n\n(18408, 8)\nWe filtered out:  17743 , corresponding to about 49 % of our initial data\n\n\nIt seems that because of our filtering, almost half of our data will be excluded from the analysis. This is a quite large percent and we may decide to re-think our filtering criteria to include more data. For example, we could substitue the missing value in the Captives column with an avarage number of captived per trip. For the purpose of our workshop, we will keep the current filtering criteria and keep our filtered DataFrame as it is.\nAt this point we obtained a “clean” DataFrame, cleaned_df, containing 18408 rows with values organised in 8 columns. We can now start diving deep in the analysis of our DataFrame, we are ready to interrogate this dataset and see which kind of story it is going to tell us."
  },
  {
    "objectID": "parts/data_analysis/ch4_data_analysis.html",
    "href": "parts/data_analysis/ch4_data_analysis.html",
    "title": "6  Analysing data",
    "section": "",
    "text": "Data Analysis\nIt is finally time to ask questions to our data. Let’s start with some simple ones regaring the time span of our dataset.\nQuestion(s)\n                        \n                    \n                    \n                        What's the year of the very first and last voyage record?How many years does our data set span?How does that compare to unfiltered data?\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Go through the \"Year of arrival at port of disembarkation\" column and look for the largest and smallest number;Repeat the same procedure for the raw data set and compare.\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    pandas method .iloc();Python functions min() and max();Python print() function.\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        arrival_year = cleaned_df.iloc[:,0], we use the method .iloc[] to select the first column     of the DataFrame cleaned_df and we store the column in the variable arrival_year;first_year = min(arrival_year)\nlast_year = max(arrival_year)\nyear_span = last_year-first_year, we use the Python functions min() and max() to compute the minimum and     maximum values of the year column storing their values in first_year and last_year,     respectively. We also compute the difference between these two values and store the result in year_span.\narrival_year = cleaned_df.iloc[:,0]\nfirst_year = min(arrival_year)\nlast_year = max(arrival_year)\nyear_span = last_year-first_year\n\nprint(first_year)\nprint(last_year)\nprint(year_span)\n\n1520.0\n1866.0\n346.0\narrival_year_raw = df.iloc[:,0]\nfirst_year_raw = min(arrival_year_raw)\nlast_year_raw = max(arrival_year_raw)\nyear_span_raw = last_year_raw-first_year_raw\n\nprint(first_year_raw)\nprint(last_year_raw)\nprint(year_span_raw)\n\n1514.0\n1866.0\n352.0\nAnswer\n                    \n                \n                \n                    Our cleaned data spans from 1520 to 1866, so 346. Comparing these numbers with our original, uncleaned, DataFrame, we can notice that the filtered data does not include the period between 1514 and 1520, so 6 years.\nWe can keep asking questions about numerical values. We focused on time in our last question, let’s focus on the number of captives this time.\nQuestion(s)\n                        \n                    \n                    \n                        How many captives have been disembarked between 1520 and 1866?How many captives have been disembarked on average per voyage?How many captives have been disembarked on average per year?How do these numbers change looking at the raw data?\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Go through the \"Captives arrived at 1st port\" column and compute the total number of captives;Divide the previous result by the number of voyages;Divide the total number of captives by the number of years;Adjust the previous estimates considering the average number of captives per voyage and the number of filtered rows.\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    pandas method .iloc();Python function sum();Python function len();Python function round();Python function print() .\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        n_captives = cleaned_df.iloc[:,6], we apply the method .iloc[] to select the     \"Captives arrived at 1st port\" column and we store the column in the variable n_captives;tot_captives = sum(n_captives), we use the Python function sum() to compute the sum     of all the values in n_captives and we stored the result in the variable tot_captivesave_cap_per_voyage = tot_captives/len(cleaned_df)\nave_cap_per_year = tot_captives/year_span, we use the total number of captives just computed to calculate the average number of captives     per vorage (dividing it by the number of rows) and per year (dividing it by the year span). We store the result     in ave_cap_per_voyage and ave_cap_per_year, respectively;filtered_rows = len(df)-len(cleaned_df)\ntot_captives_ext = tot_captives + ave_cap_per_voyage*filtered_rows\nave_cap_per_year = tot_captives/year_span, we compute the number of filtered rows subtracting the length of our original DataFrame df     by the length of our cleaned DataFrame cleaned_df. Multiplying this number by the average number of     captives per voyage (remember that in our DataFrames each row corresponds to a voyage), we obtain an estimate of the     number of captives that have not been considered as the corresponding data is missing and, as such, it has been filtered     out from our analysis. We store this estimate in the variable tot_captives_ext. We then adjust our estimate     of the number of captives per years using this value and dividing it by year_span_raw, the year span of     our original DataFrame. We store this value into the variable ave_cap_per_year_adj;print(round()), we print different results related to people using the Python function     round() that, without any other argument, approximate decimal numbers to the closest integer.\nn_captives = cleaned_df.iloc[:,6]\ntot_captives = sum(n_captives)\nave_cap_per_voyage = tot_captives/len(cleaned_df)\nave_cap_per_year = tot_captives/year_span\nprint('Total n. of captives:',tot_captives)\nprint('Average captives per voyage',round(ave_cap_per_voyage))\nprint('Average captives per year',round(ave_cap_per_year))\n\nTotal n. of captives: 5082756.0\nAverage captives per voyage 276\nAverage captives per year 14690\nfiltered_rows = len(df)-len(cleaned_df)\ntot_captives_ext = tot_captives + ave_cap_per_voyage*filtered_rows\nave_cap_per_year_adj = tot_captives_ext/year_span_raw\nprint('Extimated total n. of captives',round(tot_captives_ext))\nprint('Adjusted average captives per year', round(ave_cap_per_year_adj))\n\nExtimated total n. of captives 9981894\nAdjusted average captives per year 28358\nAnswer\n                    \n                \n                \n                    Our first estimate of the total number of captives was 5 082 756 in the time period between 1520 and 1866,  and a corresponding average of captives per year of 14 690. However, this estimate did not take into account filtered data. In our data filtering we excluded many rows, so many voyages. Almost 50% of the initial voyages has been filtered our. We therefore computed the average of captives per voyage and estimated the number of filtered out captives. The result is a number that almost double (as expected) our previous estimate: 9 981 894 people, with an average of 28 358 people traded per year.\nSo far we computed numbers, but data can be most effectively described using visualization. In our DataFrame we have information about three different locations: the place where the voyage started, the principal port of slave purchase, and the principal port of slave disembarkation. Let’s have a closer look at these locations.\nQuestion(s)\n                        \n                    \n                    \n                        Which is the town where most of the voyages started?Can we sort towns according to the number of voyages that started there?How about the other two locations, place of slave purchase and disembarkation?\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Go through the \"Voyage itinerary imputed port where began (ptdepimp) place\" column and count how many times     a town name occurs;Repeat this procedure for all the towns;Sort towns according to how many times their names show up;Make a plot with town names on the vertical axis and how many times they have been starting point for voyages in     the orizontal axis;Repeat the same procedure for the other two locations: port of slave purchase and disembarkation.\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    pandas method .iloc[]pandas method .value_counts()Python package seaborn;Python package matplotlib.pyplot;matplotlib.pyplot function .subplots();seaborn function .barplot()\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        start_port = cleaned_df.iloc[:,3], we use the pandas method .iloc[] to     select our target column. In this case it is \"Voyage itinerary imputed port where began (ptdepimp) place\", the     fourth column of our DataFrame (Python index 3). We store this column in the variable start_port;start_port_counts = start_port.value_counts(), we want to count how many times a certain town name     occurs in our column and the pandas method .value_counts() does exactly this. After     applying the method, we obtain a pandas DataFrame with a single column (a Series) having as index the     name of the town and as value the number of times that town shows up in the column start_port. We store     this result in start_port_counts;start_port_counts, we display the content of the variable start_port_counts on the screen;import seaborn as sns, we import the Python package seaborn and we give it the alias     sns. This means that, from now own, if we want to use a certain function belonging to this package, we     will call it using: sns.function_name();import matplotlib.pyplot as plt, we import the Python package matplotlib.pyplot and we     give it the alias plt;fig, new_ax = plt.subplots(nrows=1,ncols=1,figsize=(8,8)), we use the matplotlib.pyplot     function .subplots() to initialise an empty plot. .subplots() it is generally used to create     a grid of plots and its first two arguments (nrows and ncols) specify the number of rows     and columns in this grid. In our case, we just want a single plot, therefore we specify a grid having only one column     and one row. The parameter figsize specifies the size of the figure in inches. In our case     ((8, 8)) it will be 8 inches wide and 8 inches high. This function returns two different objects: a     Figure and an Axes. The Figure is the white space where we are going to put our plot, it is our plot container. The     Axes, despite its name, it is the plot itself. We did not initialise our plot yet, so the Axes object is empty, but this is     one of the standard procedure in Python: first create the space where to plot, then plot the data. Figure and Axes are     stored in the variables fig and new_ax, respectively;filter = start_port_counts &gt; 50, if we plot ALL our data, the plot is going to look overcrowded. For this     reason, we can specify a threeshold and plotting only towns that appear a number of times that is higher than our     threeshold. In our case, and in the specific case of this plot, this number will be 50.     start_port_counts &gt; 50 is a Python condition, it basically creates an array with the same size     and features of start_port_counts containing boolean values (True and False) instead of numerical values.     When True and when False? If a value in start_port_counts is higher than 50, the corresponding boolean     value will be True, otherwise it will be False. We store this boolean array in the variable filter as     this can be used as a mask to filter out only those values corresponding to True     from an array with its same dimention and features;x_data = start_port_counts[filter]\ny_data = start_port_counts.index[filter]\nsns.barplot(ax=new_ax,x=x_data,y=y_data), we use the     seaborn function .barplot() to create (guess what??) a barplot. The barplot will be     located in our just created Axes object, this is implemented using the argument ax=new_ax. We need to     specify which data to plot in the horizontal x and y axis. We want to plot town name in the     y axis and town name counts in the y axis. We also want to display only those data with more than 50 counts. We can     obtain this with x=start_port_counts[filter] and y=start_port_counts.index[filter], as the     town name is specified in the start_port_counts index and the town counts in the     start_port_counts values;We repeat the same procedure for the fifth and sixth column of our DataFrame.\nstart_port = cleaned_df.iloc[:,3]\nstart_port_counts = start_port.value_counts()\nprint(type(start_port_counts))\nstart_port_counts\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\nVoyage itinerary imputed port where began (ptdepimp) place\nLiverpool                       3227\nunknown                         2005\nLondon                          1874\nBahia, place unspecified        1815\nRio de Janeiro                  1464\n                                ... \nMangaratiba                        1\nMediterranean coast (France)       1\nCanasí                             1\nSanta Catarina                     1\nPortland                           1\nName: count, Length: 176, dtype: int64\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, new_ax = plt.subplots(nrows=1,ncols=1,figsize=(8,8))\nfilter = start_port_counts &gt; 50\nx_data = start_port_counts[filter]\ny_data = start_port_counts.index[filter]                                                                                                                                                           \nsns.barplot(ax=new_ax,x=x_data,y=y_data)\n\n&lt;Axes: xlabel='count', ylabel='Voyage itinerary imputed port where began (ptdepimp) place'&gt;\nmain_pur_port = cleaned_df.iloc[:,4]\nmain_pur_counts = main_pur_port.value_counts()\nmain_pur_counts\n\nVoyage itinerary imputed principal place of slave purchase (mjbyptimp) \nAfrica, port unspecified                                3177\nLuanda                                                  1447\nWest Central Africa and St. Helena, port unspecified    1139\nBonny                                                    853\nGold Coast, port unspecified                             787\n                                                        ... \nPetit Mesurado                                             1\nEva                                                        1\nPokesoe (Princes Town)                                     1\nSassandra                                                  1\nSugary (Siekere)                                           1\nName: count, Length: 161, dtype: int64\nfig, ax = plt.subplots(1,1,figsize=(8,8))\nfilter = main_pur_counts &gt; 200\nsns.barplot(ax=ax,x=main_pur_counts[filter],y=main_pur_counts.index[filter])\n\n&lt;Axes: xlabel='count', ylabel='Voyage itinerary imputed principal place of slave purchase (mjbyptimp) '&gt;\nmain_dis_port = cleaned_df.iloc[:,5]\nmain_dis_counts = main_dis_port.value_counts()\nmain_dis_counts\n\nVoyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\nBahia, place unspecified       1720\nRio de Janeiro                 1651\nKingston                       1576\nBarbados, place unspecified    1524\nHavana                          893\n                               ... \nFrance, place unspecified         1\nSanta Marta                       1\nDois Rios                         1\nMaceió                            1\nBonny                             1\nName: count, Length: 240, dtype: int64\nfig, ax = plt.subplots(1,1,figsize=(8,8))\nfilter = main_dis_counts &gt; 150\nsns.barplot(ax=ax,x=main_dis_counts[filter],y=main_dis_counts.index[filter])\n\n&lt;Axes: xlabel='count', ylabel='Voyage itinerary imputed principal port of slave disembarkation (mjslptimp) place'&gt;\nAnswer\n                    \n                \n                \n                    The ports where most of the voyages took place are Liverpool and London (with a considerable percent being unkown). Most of the places where slaves have been purchased remain unkown, the second most occuring purchanse place is Luanda (mid Africa). The most frequent places of disimbarkation are Bahia and Rio de Janeiro, on the opposite side of the Atlantic Ocean at a similar latitude.\nLet’s try to make now a different type of visualization, a time series, i.e. a plot where we see how parameters change over time\nQuestion(s)\n                        \n                    \n                    \n                        How does the total number of disembarkated captives change over time?\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Consider the data set columns containing information about time and number of captivesGroup data rows by yearSum the number of captives per yearMake a plot with years (time) on the x axis and number of captives on the y axisLocate the minimum and the maximum of captives per year\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    pandas method .groupby();parnas method .mean();Python package matplotlib.pyplot;Python package seaborn;matplotlib.pyplot function .subplots();seaborn function .lineplot();matplotlib.pyplot function .grid();pandas method .idxmin();pandas method .idxmax();Python function print()\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        col_to_group = \"Year of arrival at port of disembarkation\"\ncol_to_sum = \"Captives arrived at 1st port\", we select the two columns in the DataFrame we are interested in, the one containing the time     information and the one containing the number of captives. We store the name of these two columns in the     variables col_to_group and col_to_sum, respectively;df_per_year = cleaned_df.groupby(col_to_group)[col_to_average].sum(), we first apply the     method .groupby() to our cleaned DataFrame. .groupby() does exactly what it says, it     groups data according to a certain column (in our case the col_to_group column), this means that any     other method we are going to apply from now own, it will operate on groups of rows instead of the entire DataFrame.     For example, if we apply a method perfoming summation or average on a DataFrame grouped by year, python will     compute the sum or the average on those groups of data. In a DataFrame there are many columns containing values     to sum and average, so which column is Python going to choose? We need to specify it, in our case we do that     with [col_to_sum], where the column to sum contains information about the number of captives. We     finally apply the method .sum() to compute the sum of number of captives on DataFrame rows grouped     by year. We store the result in the variable df_per_year;sum_df_per_year, we display our last result on the screen;fig, my_ax = plt.subplots(nrows=1,ncols=1,figsize=(8,8)), we use the function .subplots()     to create a single squared empty plot with size 8 inches. We obtain a Figure object (the container of our plot) and     a Axes object (an empty plot). We store these two objects in the variables fig and my_ax;sns.lineplot(ax=my_ax,x=sum_df_per_year.index,y=sum_df_per_year), we use the method     .lineplot() to create a line plot (data points connected by a line). We plot the index of the Series     (a Series is a DataFrame with only one column) df_per_year on the x axis and its values on the y axis.plt.grid(), we use the function .grid() to overplot a grid on our plot, with the purpose of     better describe the data by eye;max_index = df_per_year.idxmax();\nmin_index = df_per_year.idxmin();\nmin_year = df_per_year[min_index]\nmax_year = df_per_year[max_index]\nprint('Min. n. of captives per year:', min_year,'on',min_index);\nprint('Max. n. of captives per year:', max_year,'on',max_index)., we apply the method .idxmax() to the Series df_per_year to get the index     corresponding to the maximum of its values. Remember that df_per_year indeces are year and its values     are the sum of number of captives in that year. Therefore, obtaining the index corresponding to the maximum value     means determining the year having the maximum number of captives. We store this result in the variable     max_index. We can use this index to retrieve the corresponding value:     df_per_year[max_index]. We repeat the same procedure for the minimum value using     .idxmin(). We finally print both results on the screen with the function print().\ncol_to_group = 'Year of arrival at port of disembarkation'\ncol_to_sum = 'Captives arrived at 1st port'\ndf_per_year = cleaned_df.groupby(col_to_group)[col_to_sum].sum()\nprint(df_per_year.shape)\ndf_per_year\n\n(298,)\n\n\nYear of arrival at port of disembarkation\n1520.0       44.0\n1526.0      115.0\n1527.0       46.0\n1532.0      589.0\n1534.0      354.0\n           ...   \n1862.0    11407.0\n1863.0     6739.0\n1864.0     3298.0\n1865.0      795.0\n1866.0      700.0\nName: Captives arrived at 1st port, Length: 298, dtype: float64\nfig, ax = plt.subplots(1,1,figsize=(8,8))\nsns.lineplot(ax=ax,x=df_per_year.index,y=df_per_year)\nplt.grid()\n\n/opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\nmax_index = df_per_year.idxmax()\nmin_index = df_per_year.idxmin()\nmin_year = df_per_year[min_index]\nmax_year = df_per_year[max_index]\n\nprint('Min. n. of captives per year:', min_year,'on',min_index)\nprint('Max. n. of captives per year:', max_year,'on',max_index)\n\nMin. n. of captives per year: 2.0 on 1538.0\nMax. n. of captives per year: 79472.0 on 1829.0\nsolution = 'The total number of captives is almost constant up to 1650, with the exception of two peaks around 1600 \\\nand 1622. The number increases steadily up to 1800 and decreases afterwords. The times series is not smooth, but it proceeds \\\nby low and high peek. The number of captives per year reaches its maximum on 1829 with almost 80 thousands slaves traded \\\nthat year. The minimum is 2 captives on 1538.'\nquestion_box(solution=solution)\n\n\n            \n                \n                    \n                    \n                        Answer\n                    \n                \n                \n                    The total number of captives is almost constant up to 1650, with the exception of two peaks around 1600 and 1622. The number increases steadily up to 1800 and decreases afterwords. The times series is not smooth, but it proceeds by low and high peek. The number of captives per year reaches its maximum on 1829 with almost 80 thousands slaves traded that year. The minimum is 2 captives on 1538.\nTime series are very interesting to describe the trends of phenomema at different scale. Our plot ticks are separated by 50 years, this is fine to visualise trends over centuries, but we cannot see what’s happening on decades.\nQuestion(s)\n                        \n                    \n                    \n                        What's the disembarked captive trend between 1700 and 1750?\n                    \n            \n        \n            \n                \n                    \n                    \n                        What to do?\n                    \n                \n                \n                    Go through the steps for making the previous plot filtering data spanning between 1700 and 1750\n                \n                \n        \n            \n                \n                    \n                    \n                        (Python) Tools\n                    \n                \n                \n                    Python package matplotlib.pyplot;Python package seaborn;matplotlib.pyplot function .subplots();seaborn function .lineplot();matplotlib.pyplot function .grid().\n                            \n                \n        \n            \n                \n                    \n                    \n                        Coding\n                    \n                \n                \n                    \n                        time_filter = (df_per_year.index &gt; 1700) & (df_per_year.index &lt; 1750), we start using the pandas Series df_per_year. This     series contains information about the total number of captives per year. Years are contained in its index, number of captives in its values. We want to select     data between 1700 and 1750, so with years larger than 1700 and smaller than 1750. We do that using two Python conditions: df_per_year.index &gt; 1700 and     df_per_year.index &lt; 1750. As we want these two conditions to be true at the same time, we specify both of them with the symbol & in     between. In Python, the commertial & indicates a logical AND, this means that the condition before and after the AND must both be simultaneously true. We store     this condition into the variable time_filter;fig, ax = plt.subplots(1,1,figsize=(8,8))\nx_data = df_per_year.index[time_filter]\ny_data = df_per_year[time_filter]\nsns.lineplot(ax=ax,x=x_data,y=y_data)\nplt.grid(), as we did in the previous task, we plot total number of captives versus years. This time, instead of using our full array, we want to plot only     a certain time range. This has been specified in the condition stored into time_filter. Therefore, compared to our previous plot, we are going to     user df_per_year.index[time_filter] and df_per_year[time_filter] instead of df_per_year.index and df_per_year, respectively.\ntime_filter = (df_per_year.index &gt; 1700) & (df_per_year.index &lt; 1750)\nfig, ax = plt.subplots(1,1,figsize=(8,8))\nx_data = df_per_year.index[time_filter]\ny_data = df_per_year[time_filter]\nsns.lineplot(ax=ax,x=x_data,y=y_data)\nplt.grid()\n\n/opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead."
  },
  {
    "objectID": "parts/data_analysis/ch4_data_analysis.html#previous-steps",
    "href": "parts/data_analysis/ch4_data_analysis.html#previous-steps",
    "title": "6  Analysing data",
    "section": "Previous steps",
    "text": "Previous steps\n\nimport pandas as pd\ndata_file = 'data/data.csv'\ndf = pd.read_csv(data_file)\ncleaned_df = df.drop('VOYAGEID2',axis=1).dropna(subset=['Captives arrived at 1st port']).fillna(\"unknown\")\ncleaned_col_names = cleaned_df.columns\ncleaned_df.head(10)\n\n\n\n\n\n\n\n\nYear of arrival at port of disembarkation\nVoyage ID\nVessel name\nVoyage itinerary imputed port where began (ptdepimp) place\nVoyage itinerary imputed principal place of slave purchase (mjbyptimp)\nVoyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\nCaptives arrived at 1st port\nCaptain's name\n\n\n\n\n0\n1714.0\n16109\nFreeke Gally\nBristol\nunknown\nKingston\n283.0\nNeale, Alexander\n\n\n2\n1714.0\n16111\nJacob\nBristol\nunknown\nKingston\n130.0\nNicholls, Philip\n\n\n3\n1714.0\n16112\nJason Gally\nBristol\nunknown\nPort Royal\n278.0\nPlummer, John\n\n\n5\n1714.0\n16114\nMercy Gally\nBristol\nAfrica, port unspecified\nBarbados, place unspecified\n190.0\nScott, John\n\n\n6\n1714.0\n16115\nMermaid Gally\nBristol\nCape Verde Islands\nKingston\n72.0\nBanbury, John&lt;br/&gt; Copinger, James\n\n\n8\n1714.0\n16117\nPeterborough\nBristol\nAfrica, port unspecified\nBarbados, place unspecified\n200.0\nShawe, John&lt;br/&gt; Martin, Joseph\n\n\n9\n1713.0\n16118\nResolution\nBristol\nGold Coast, port unspecified\nBarbados, place unspecified\n255.0\nWilliams, Charles\n\n\n10\n1714.0\n16119\nRichard and William\nBristol\nunknown\nPort Royal\n55.0\nBeckham, George&lt;br/&gt; Spring, Martin\n\n\n11\n1713.0\n16120\nRotchdale Gally\nBristol\nAfrica, port unspecified\nBarbados, place unspecified\n96.0\nHitchings, John\n\n\n12\n1714.0\n16121\nTunbridge Gally\nBristol\nAfrica, port unspecified\nBarbados, place unspecified\n200.0\nSkinner, Peter\n\n\n\n\n\n\n\n\nprint(\"Index) Column name\") \nfor i,name in enumerate(cleaned_df.columns): \n    print(i,\")\",name) \n\nIndex) Column name\n0 ) Year of arrival at port of disembarkation\n1 ) Voyage ID\n2 ) Vessel name\n3 ) Voyage itinerary imputed port where began (ptdepimp) place\n4 ) Voyage itinerary imputed principal place of slave purchase (mjbyptimp) \n5 ) Voyage itinerary imputed principal port of slave disembarkation (mjslptimp) place\n6 ) Captives arrived at 1st port\n7 ) Captain's name"
  },
  {
    "objectID": "parts/supplements/summary.html#databases-and-data-analysis",
    "href": "parts/supplements/summary.html#databases-and-data-analysis",
    "title": "Summary",
    "section": "Databases and Data Analysis",
    "text": "Databases and Data Analysis\nIn the first part of the workshop, we conducted an interactive exercise to experience the main challenges associated with converting unstructured data into structured, organized, tabular (and generally more mathematical) data. Having structured, machine-readable data is essential for proper data analysis. It is important to note that database creation, which marks the beginning of every data analysis process, can be affected by errors, missing data, and biases driven by human decisions.\nIn the second part of the workshop, we focused on data analysis. Data analysis is often only considered in relation to programming tools. However, we aimed to emphasize the fundamental principles of data analysis, emphasizing that analyzing data essentially involves querying a dataset. While some information may be easy to retrieve, other information may be hidden or require assumptions and speculation. Python (or any other programming language) is simply a tool to translate our questions into a machine-readable form.\nRegardless of our data analysis process and the tools we use, it is crucial to describe and document our choices so that other researchers can reproduce the entire workflow that led to our conclusions."
  },
  {
    "objectID": "parts/supplements/summary.html#general-data-analysis-workflow",
    "href": "parts/supplements/summary.html#general-data-analysis-workflow",
    "title": "Summary",
    "section": "General data analysis workflow",
    "text": "General data analysis workflow\n\nDefine a Research question:\n\nUnderstand the problem or question you are trying to address;\nClearly define the goals, objectives, and sub-task to answer a research question.\n\nCollect/Organise Data:\n\nCollect relevant data from various sources;\nEnsure data quality, address any missing or inconsistent data, ensure proper data structure.\n\nClean Data:\n\nClean and preprocess the data to handle missing values, outliers, and errors;\nStandardize or normalize data formats if necessary.\n\nExplore Data:\n\nExplore the data using statistical and visual methods;\nIdentify patterns, trends, and relationships in the data.\n\n(Model):\n\nSelect appropriate models based on the analysis goals;\nEvaluate the model’s performance using metrics relevant to the analysis;\nFine-tune the model if necessary.\n\nInterpret Data:\n\nInterpret the results of the analysis in the context of the initial research question;\nDraw conclusions and make recommendations based on the findings.\n\nVisualization and Reporting:\n\nCreate visualizations to communicate key findings;\nPrepare a comprehensive report summarizing the analysis process, results, and insights."
  },
  {
    "objectID": "parts/supplements/what-next.html",
    "href": "parts/supplements/what-next.html",
    "title": "What’s next?",
    "section": "",
    "text": "Congratulations! If you are reading these few lines you survived our workshop on analysing cultural data (and you even went through the documentation!). Our workshop was only an introduction, it would have been impossible to cover everything related to analysing cultural data in only four hours, but we hope you now have a general overview of how data analysis is performed and of all the caveats related to data base creation and analysis.\nWhat’s next? You can build on top of what we have discussed during the workshop. Here there is a list of possible further steps, good luck!\n\nMore data: try to obtain more data in .csv form and perform the data analysis on this new data. You can either get new data in the SlaveVoyages website or download any data in .csv format. Just remember to download it in the “data” directory and to change data_file into “data/your_file.csv”. You can also use data in a different format (like excel sheet for example) and read it with the corresponding pandas tool;\nLearning more about Python: if you know nothing about programming and Python, you might consider to invest some time for learning about it. The Utrecht University Library and the Centre for Digital Humanities (CDH) offer free Python courses: have a look at the Research Data Management (RDM) workshop page and at the CDH workshop page;\nMore data questioning: you can ask your own questions to data and find a way to implement that in Python (or any other programming language you are going to use)"
  },
  {
    "objectID": "parts/supplements/glossaries.html#data-glossary",
    "href": "parts/supplements/glossaries.html#data-glossary",
    "title": "Glossaries",
    "section": "Data Glossary",
    "text": "Data Glossary\n\nControlled vocabularies\nStandardised sets of terms or phrases used to ensure consistency and accuracy in categorising and retrieving information.\n\n\nData cleaning\nThe process of identifying and correcting errors, inconsistencies, and inaccuracies in a dataset to improve its quality and reliability.\n\n\nData harmonisation\nThe process of integrating and standardising data from different sources or formats to ensure consistency and compatibility for analysis or other purposes.\n\n\nData models\nAbstract representations defining the structure, relationships, and constraints of data within a system or database.\n\n\nEnrichment\nThe process of enhancing or augmenting existing data with additional information to improve its quality, usability, or value.\n\n\nGraph database\nA database structured around graph theory, where data entities are represented as nodes and their relationships as edges, facilitating complex and interconnected data querying.\n\n\nID\nIdentification or identifier used to uniquely distinguish an entity within a system.\n\n\nNormalisation\nThe process of organising data in a database to reduce redundancy and dependency by dividing large tables into smaller ones and defining relationships between them.\n\n\nRelational database\nA type of database management system (DBMS) organised around tables and relationships, adhering to the principles of the relational model."
  },
  {
    "objectID": "parts/supplements/glossaries.html#programming-glossary",
    "href": "parts/supplements/glossaries.html#programming-glossary",
    "title": "Glossaries",
    "section": "Programming glossary",
    "text": "Programming glossary\n\nCode\nCode is like a set of instructions that tells the computer what to do. It’s similar to a recipe for the computer. The instructions for a specific task may very according to the programming language you use, that is why you also usually specify the language you are using (e.g. “Python code”).\n\n\ncsv\nCSV (coma separated values) is a way to store information, like making lists. It’s a simple way to organize data, like names and ages, using commas. A file containing data organised in this way, has usually the extention “.csv”. Even if the word “coma” is present in the acronym, data can be also separated by other symbols such as “;” or “:” and still be contained in a .csv file.\n\n\nDataFrame\nA DataFrame is a Python object included in the pandas library. It is basically a table where information is organised in rows and columns. Every DataFrame row has an index that can be either a numeric value or a string (i.e. a label)\n\n\nInitialisation\nInitialise basically means getting things ready. It’s the starting point before using something in a program. Initialising a variable, in particular, means assigning a value to it for the first time:\n\n# We initialise the variable name and age for the first time with a string (a word) and a number\nname = 'Stefano'\nage = 28\n\n\n\nLibrary\nSee package\n\n\nLoop\nA loop is like a computer doing something over and over again. It’s a way to repeat a task multiple times.\n\n\nObject\nAn object is like a thing in the computer’s world. It has characteristics and things it can do. For example, a dog can have a name and bark.\n\n\nPackage\nA library or package is like a toolbox with ready-made tools. It’s a collection of helpful code (objects and functions) that programmers can use to make their work easier. Packages are made by the programming community and are usually organised according to certain specific tasks. You may find packages specific for time series analysis, text analysis, satellite image analysis, etc. The advantage of using a package is that you do not have to spend time and energy in finding solutions to problems already tackled by other people. Packages do not usually come automatically with the basic programming language installation (so to optimize space), but need to first be downloaded and imported. You may think at packages as building tools. Downloading a package would be similar to buying them from the shop and importing them is similar to get them ready to work (you do not need all your tools ALL the time for ANY house job, right?)\n\n\nSeries\nA Series is a DataFrame with a single column. Like DataFrames, a Series is an object belonging to the pandas library. Series rows, like in a DataFrame, have indices that can be either values or labels.\n\n\nVariable\nA variable is like a box where you can keep and change information. It is a container for numbers or words."
  },
  {
    "objectID": "parts/supplements/resources.html#datasets",
    "href": "parts/supplements/resources.html#datasets",
    "title": "Resources",
    "section": "Datasets",
    "text": "Datasets\n\nSlaveVoyages website;\nCSV exports of the Getty Provenance Index GitHub page;\nNational Gallery of Art Open Data GitHub page."
  },
  {
    "objectID": "parts/supplements/resources.html#programming",
    "href": "parts/supplements/resources.html#programming",
    "title": "Resources",
    "section": "Programming",
    "text": "Programming\n\nPython webpage;\nPandas (Python Data Analysis package) webpage;\nMatplotlib (Python visualization package) webpage;\nSeaborn webpage (Python visualization package) webpage."
  }
]