[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data exploration toolkit for cultural data: structure, clean, visualize, and run a preliminary analysis",
    "section": "",
    "text": "Welcome!\nWelcome to the Cultural Data Exploration Toolkit! Throughout this workshop, you’ll explore the nuances of data-driven analysis, from constructing your dataset and formulating research inquiries to learning data visualisation techniques. Along this journey, you’ll contemplate the creation of personalised data models tailored to your research queries, navigate potential biases within datasets, and, importantly, learn how to effectively interrogate, explore, and analyse gathered information to generate visualisations."
  },
  {
    "objectID": "parts/preliminaries/set_up.html#running-the-analysis-during-the-workshop",
    "href": "parts/preliminaries/set_up.html#running-the-analysis-during-the-workshop",
    "title": "Setting up",
    "section": "Running the analysis during the workshop",
    "text": "Running the analysis during the workshop\n\nWrite the following link to your browser: https://tinyurl.com/cdw0324 ;\nWait for the instructor to give you access to the shared working environment;\nClick on the “New” button on the top left corner of your Google Drive page (just below the Google Drive icon), select “others”, and then Google collaboratory. Be sure that the just created jupyter notebook is in the project directory (the one containing your data directory). Rename the just created file with your name, e.g. Stefano.ipynb;\nOpen the jupyter notebook file called “analysis_in_class”, copy the content of the first cell and paste it in the first cell of your own file."
  },
  {
    "objectID": "parts/preliminaries/set_up.html#running-the-analysis-locally-on-your-own-computer",
    "href": "parts/preliminaries/set_up.html#running-the-analysis-locally-on-your-own-computer",
    "title": "Setting up",
    "section": "Running the analysis locally on your own computer",
    "text": "Running the analysis locally on your own computer\n\nInstall python and jupyter notebook. For installation and setup we point at the “Introduction to Python & Data” Installation & Setup page;\nCreate an empty directory called “cultural_data_analysis” (or any other name you prefer);\nInside the just created directory, create another directory called “data”;\nClick on this link, this will open a GitHub page. On the toolbar (on the right of the buttons “Raw” and copy), you will find the button for downloading the .csv file containing the data we used during our workshop. Download the file inside the just created data directory;\nClick on this link, this will open the Jupyter-notebook in one of the instructor google colab environment containing the full analysis performed during the workshop. You can access the file only if you followed the workshop. At this point, you will just need to click on the “File” tab, select “download” (almost at the very end of the menu), select the “Download .ipynb” option, and download the file inside your project directory (the one also containing the data directory);\nOpen the jupyter notebook and have fun!\n\n!!! WARNING !!! In the first cell of the Jupyter-notebook you downloaded, the instructors wrote specific Python instructions to make the notebook work in their Google colab environment. In your case, the only thing you need to do is to find out the full path of your project directory (the one containing the notebook and the data directory you just created) and writing this path instead of “working_directory”, as described in the comments of the first cell."
  },
  {
    "objectID": "parts/preliminaries/set_up.html#running-the-analysis-from-remote-in-your-google-colab",
    "href": "parts/preliminaries/set_up.html#running-the-analysis-from-remote-in-your-google-colab",
    "title": "Setting up",
    "section": "Running the analysis from remote in your Google colab",
    "text": "Running the analysis from remote in your Google colab\n\nIn your personal Google Drive page, create a new directory called “cultural_data_analysis” (or any other name you prefer). For doing that, click on the “New” button on the top left corner of your browser (just below the Drive icon) and select directory;\nGo inside the just created directory and create another directory called “data”;\nClick on this link, this will open a GitHub page. On the toolbar (on the right of the buttons “Raw” and copy), you will find the button for downloading the .csv file containing the data we used during our workshop. Download the file inside the just created data directory;\nClick on this link, this will open the Jupyter-notebook in google colab containing the full analysis performed during the workshop. You can access the file only if you followed the workshop. At this point, you will just need to click on the “File” tab, then click on “download” (almost at the very end of the menu), select the “Download .ipynb” option, and download the file inside your project directory (the one also containing the data directory). Alternatively, if you want to start from scratch creating a black jupyter notebook, click again on the “New” button, select “others”, and then Google collaboratory. Be sure that the just created jupyter notebook is in your project directory (the one containing your data directory);\n\n!!! WARNING !!! In the first cell of the Jupyter-notebook you downloaded, the instructors wrote specific instructions to make the notebook work in their Google colab environment. In your case, the only thing you need to do is to change the value of the variable work_dir in the first cell from ‘/det_cultural_data’ to ‘/your_dir_name’. If the name of your directory is det_cultural_data, you do not need to change anything."
  },
  {
    "objectID": "parts/supplements/summary.html#databases-and-data-analysis",
    "href": "parts/supplements/summary.html#databases-and-data-analysis",
    "title": "Summary",
    "section": "Databases and Data Analysis",
    "text": "Databases and Data Analysis\nIn the first part of the workshop we ran through an interactive exercise to experience the main problems related to turning unstructured data into structured, organised, tabular (and in general more mathematical) data. Having food structured, machine readable, data is essential for proper data analysis. It is extremely important to keep in mind that database creation, the process at the very beginning of every data analysis, can be affected by errors, missing data, and human-decision-driven biases.\nIn the second part of the worshop we focused on data analysis. Performing data analysis is something too often considered only in relation to programming tools. We tried to focus more on the basic principles of data analysis, highlighting the fact that analysing data basically means interrogating a data set. Some information can be very easy to retrieve, other information is hidden or requires assumptions and speculations. Python (or any other programming language) is merely a tool to translate our questions into a machine readable form. Whatever is our data analysis process and the tools we use to perform it, it is extremely important to describe and document our choices, so that other researchers can reproduce the entire workflow that led to our conclusions."
  },
  {
    "objectID": "parts/supplements/summary.html#general-data-analysis-workflow",
    "href": "parts/supplements/summary.html#general-data-analysis-workflow",
    "title": "Summary",
    "section": "General data analysis workflow",
    "text": "General data analysis workflow\n\nDefine a Research question:\n\nUnderstand the problem or question you are trying to address;\nClearly define the goals, objectives, and sub-task to answer a research question.\n\nCollect/Organise Data:\n\nCollect relevant data from various sources;\nEnsure data quality, address any missing or inconsistent data, ensure proper data structure.\n\nClean Data:\n\nClean and preprocess the data to handle missing values, outliers, and errors;\nStandardize or normalize data formats if necessary.\n\nExplore Data:\n\nExplore the data using statistical and visual methods;\nIdentify patterns, trends, and relationships in the data.\n\n(Model):\n\nSelect appropriate models based on the analysis goals;\nEvaluate the model’s performance using metrics relevant to the analysis;\nFine-tune the model if necessary.\n\nInterpret Data:\n\nInterpret the results of the analysis in the context of the initial research question;\nDraw conclusions and make recommendations based on the findings.\n\nVisualization and Reporting:\n\nCreate visualizations to communicate key findings;\nPrepare a comprehensive report summarizing the analysis process, results, and insights."
  },
  {
    "objectID": "parts/supplements/what-next.html",
    "href": "parts/supplements/what-next.html",
    "title": "What’s next?",
    "section": "",
    "text": "Congratulations! If you are reading these few lines you survived our workshop on analysing cultural data (and you even went through the documentation!). Our workshop was only an introduction, it would have been impossible to cover everything related to analysing cultural data in only four hours, but we hope you now have a general overview of how data analysis is performed and of all the caveats related to data base creation and analysis.\nWhat’s next? You can build on top of what we have done during the workshop. Here there is a list of possible further steps, good luck!\n\nMore data: try to obtain more data in .csv form and perform the data analysis on this new data. You can either get new data in the SlaveVoyages website or download any data in .csv format. Just remember to download it in the “data” directory and to change data_file into “data/your_file.csv”. You can also use data in a different format (like excel sheet for example) and read it with the corresponding pandas tool;\nLearning more about Python: if you know nothing about programming and Python, you might consider to invest some time for learning about it. The Utrecht University Library and the Centre for Digital Humanities (CDH) offer free Python courses: have a look at the Research Data Management (RDM) workshop page and at the CDH workshop page;\nMore data questioning: you can ask your own questions to data and find a way to implement that in Python (or any other programming language you are going to use)"
  },
  {
    "objectID": "parts/supplements/resources.html#datasets",
    "href": "parts/supplements/resources.html#datasets",
    "title": "Resources",
    "section": "Datasets",
    "text": "Datasets\n\nSlaveVoyages website;\nCSV exports of the Getty Provenance Index GitHub page;\nNational Gallery of Art Open Data GitHub page."
  },
  {
    "objectID": "parts/supplements/resources.html#programming",
    "href": "parts/supplements/resources.html#programming",
    "title": "Resources",
    "section": "Programming",
    "text": "Programming\n\nPython webpage;\nPandas (Python Data Analysis package) webpage;\nMatplotlib (Python visualization package) webpage;\nSeaborn webpage (Python visualization package) webpage."
  }
]